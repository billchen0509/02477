{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.special import gammaln\n",
    "\n",
    "from PIL import Image\n",
    "from exercise10 import Grid2D\n",
    "from exercise10 import VariationalGMM\n",
    "from exercise10 import plot_std_dev_contour\n",
    "from exercise10 import PCA_dim_reduction\n",
    "\n",
    "# plotting and style stuff\n",
    "snb.set_style('darkgrid')\n",
    "colors = snb.color_palette()\n",
    "snb.set_theme(font_scale=1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to become familiar with variational inference and the coordinate ascent variational inference (CAVI) algorithm in particular. First, we will study CAVI in the context of a simple model, where we will derive all the steps in the algorithm explicitly. Next, we will study Bayesian inference for the Gaussian Mixture model using variational inference and here we will focus more on the application. Finally, you will also become familiar with the Dirichlet and Wishart distributions. \n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: The coordinate ascent variational inference algorithm\n",
    "- Part 2: Bayesian Gaussian Mixture Model\n",
    "- Part 3: Variational inference for the Gaussian mixture model\n",
    "- Part 4: Analyzing a toy data set\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n",
    "**Note**: For this exericise, you will need the python module called `PIL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  The coordinate ascent variational inference algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models of practical interest leads to intractable posterior distributions, and therefore, we often need to resort to approximations. This week we will study the framework of **variational inference** as an alternative to MCMC methods. The goal of variational inference is to minimize the discrepancy between an approximation $q \\in \\mathcal{Q}$ and a target distribution of interest $p$, where the discrepancy is measured using a **divergence**. In this course, we will focus on the Kullback-Leibler (KL) divergence, such that\n",
    "$$\\begin{align*}\n",
    "q^* = \\arg\\min\\limits_{q \\in \\mathcal{Q}} \\text{KL}[q||p],\n",
    "\\end{align*}$$\n",
    "where $q^* \\in \\mathcal{Q}$ is the **optimal approximation** within a given **variational family** $\\mathcal{Q}$ and $\\text{KL}[q||p]$ is the KL-divergence between $q$ and $p$. We optimize over the set of distributions in the variational family and hence, the size of the variational family controls the quality of the approximation. Since probability distributions are functions, we optimized over a function space, which is why this framework is called **variational** inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study variational inference, we will first use it to approximate the posterior distribution of a simple toy model.  Consider a dataset given by $\\mathcal{D} = \\left\\lbrace x_1, x_2, \\dots, x_N\\right\\rbrace$, where $x_n \\in \\mathbb{R}$. We assume a Gaussian likelihood for the data, i.e.\n",
    "$$\\begin{align*}\n",
    "p(\\mathcal{D}|\\mu, \\tau) = \\prod_{n=1}^N \\mathcal{N}(x_n|\\mu, \\tau^{-1}).\n",
    "\\end{align*}$$\n",
    "\n",
    "This is equivalent to the likelihood linear regression model that only contains an intercept-term. Our goal is to estimate the posterior distribution for both the mean $\\mu \\in \\mathbb{R}$ and the precision $\\tau > 0$. First, we impose prior distributions over $\\mu$ and $\\tau$:\n",
    "$$\\begin{align*}\n",
    "p(\\mu, \\tau) &= \\mathcal{N}(\\mu|\\mu_0, (\\lambda_0\\tau)^{-1})\\text{Gamma}(\\tau|a_0, b_0),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mu_0 \\in \\mathbb{R}$ and $\\lambda_0, a_0, b_0 > 0$ are fixed hyperparameters and \n",
    "$$\\begin{align*}\n",
    "\\text{Gamma}(\\tau|a_0, b_0) = \\frac{1}{\\Gamma(a_0)}b_0^{a_0} \\tau^{a_0 - 1}\\exp(-b_0 \\tau).\n",
    "\\end{align*}$$\n",
    " \n",
    " Gaussian priors are common for **location** parameters, and Gamma priors can be used for **precision parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a synthetic dataset and visualize the exact posterior, $p(\\mu, \\tau|\\mathcal{D})$. To do that, we will re-use the `Grid2D`-class from week 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement log PDFs for Gaussian and gamma distributions\n",
    "log_npdf = lambda x, m, v: -0.5*np.log(2*np.pi*v) - 0.5*(x-m)**2/v\n",
    "log_gamma = lambda x, a0, b0: -gammaln(a0) + a0*np.log(b0) + (a0-1)*np.log(x) - b0 *x\n",
    "\n",
    "# create synthetic data set\n",
    "np.random.seed(0)\n",
    "N = 20\n",
    "x = np.random.normal(1, 0.5*np.pi, size=N)\n",
    "\n",
    "# specify hyperparameters of the model\n",
    "lambda0 = 1.\n",
    "mu0 = -1.\n",
    "a0 = 1.\n",
    "b0 = 1.\n",
    "\n",
    "# implement exact posterior up to a constant (i.e. the joint distribution)\n",
    "def exact_posterior(mu, tau):\n",
    "    \"\"\" evaluate and  returns the exact posterior distribution p(mu, tau|D) up to a constant \"\"\"\n",
    "    log_lik = log_npdf(x, mu, 1/tau)\n",
    "    log_prior = log_gamma(tau, a0, b0) + log_npdf(mu, mu0, 1./(lambda0*tau))\n",
    "    return log_lik.sum(axis=-1)  + log_prior.sum(axis=-1)\n",
    "\n",
    "# define grid for plotting\n",
    "mus = np.linspace(0, 3, 100)\n",
    "taus = np.linspace(1e-7, 1.25, 101)\n",
    "exact_posterior_grid = Grid2D(mus, taus, exact_posterior, name='Exact posterior')\n",
    "\n",
    "# visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "exact_posterior_grid.plot_contours(ax, f=np.exp)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This simple model is a **conjugate model** and hence, we can exact derive the posterior distribution analytically.  However, to become familiar with variational inference, we will approximate the posterior distribution $p(\\mu, \\tau|\\mathcal{D}) \\approx q(\\mu, \\tau)$ using a variational approximation $q(\\mu, \\tau)$. Specially, we will assume a factorized variational family:\n",
    "$$\\begin{align*}\n",
    "q(\\mu, \\tau) = q(\\mu)q(\\tau).\n",
    "\\end{align*}$$\n",
    "That is, we choose $\\mathcal{Q}$ to be the set of all distributions $q(\\mu, \\tau)$, where $\\mu$ and $\\tau$ are **independent**. However, we do **not** assume anything about the shape of $q(\\mu)$ or $q(\\tau)$. \n",
    "\n",
    "We will use the **coordinate ascent variational inference** (CAVI) algorithm to compute the optimal factors $q(\\mu)$ and $q(\\tau)$, which states that:\n",
    "$$\\begin{align*}\n",
    "\\ln q^*(\\mu) &= \\mathbb{E}_{q(\\tau)} \\left[\\ln p(\\mathcal{D}, \\mu, \\tau)\\right] + \\text{constant}\\\\\n",
    "\\ln q^*(\\tau) &= \\mathbb{E}_{q(\\mu)} \\left[\\ln p(\\mathcal{D}, \\mu, \\tau)\\right] + \\text{constant},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $p(\\mathcal{D}, \\mu, \\tau)$ is the joint distribution for the model. Before computing the expectations above, we will prepare the expression for the joint distribution,  which is the central object when working with variational inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: Show that the logarithm of the joint distribution is given by\n",
    "$$\\begin{align*}\n",
    "\\ln p(\\mathcal{D}, \\mu, \\tau) = \\left[a_0 + \\frac{N+1}{2} - 1 \\right]\\ln \\tau  -  \\left[b_0 +\\frac{ N\\bar{x^2}}{2} -  \\mu (N\\bar{x} + \\lambda_0 \\mu_0) + \\frac12 \\mu^2  (N + \\lambda_0)-  \\frac{\\lambda_0 }{2}\\mu_0^2\\right]\\tau + K,\n",
    "\\end{align*}$$\n",
    "where $K$ is an additive constant independent of $\\mu, \\tau$ and we have defined $N\\bar{x} = \\sum_{n=1}^N x_i$, and $N\\bar{x^2} = \\sum_{n=1}^N x^2_i$ to simplify the expressions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: Show that $\\ln q^*(\\mu) =   \\mu (N\\bar{x} + \\lambda_0 \\mu_0)\\mathbb{E}_{q(\\tau)} \\left[\\tau \\right] - \\frac12 \\mu^2  (N + \\lambda_0)\\mathbb{E}_{q(\\tau)} \\left[\\tau \\right]+\\text{const}$\n",
    "\n",
    "*Hints: Start with the definition for $\\ln q^*(\\mu)$. Insert the expression for the log joint distributions and absorb all terms independent of $\\mu$ into the additive constant.*\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: Argue that $q^*(\\mu) = \\mathcal{N}(\\mu| m, v)$ **must** be a Gaussian distribution and identify its mean $m$ and variance $v$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**: Show that $\\ln q^*(\\tau)$ has the functional form of a Gamma distribution, i.e. $q^*(\\tau) = \\text{Gamma}(\\tau|a,b)$. Identify the parameters $a,b$.\n",
    "\n",
    "*Hint: The functional form of a Gamma-distribution is $\\ln \\text{Gamma}(\\tau|a,b) = (a-1)\\log(\\tau)-b\\tau + K$.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now shown that\n",
    "\n",
    "$$\\begin{align*}\n",
    "q^*(\\mu) &= \\mathcal{N}(\\mu|m, v)\\\\\n",
    "q^*(\\tau) &= \\text{Gamma}(\\tau|a,b),\n",
    "\\end{align*}$$\n",
    "\n",
    "where we refer to $m, v, a, b$ as **variational parameters**.\n",
    "\n",
    "We can now compute the optimal parameters for $m, v, a, b$ using the following steps:\n",
    "- Initialize $q(\\mu)$ and $q(\\tau)$ by initializing $m, v, a, b$\n",
    "- Do until convergence or maximum number of iterations\n",
    "\n",
    "    - Update $q^*(\\mu)$ by computing $m, v$\n",
    "\n",
    "    - Update $q^*(\\tau)$ by computing $a, b$\n",
    "\n",
    "\n",
    "**Task 1.5**: Complete the implementation of the function `fit_approx` in the class `VariationalApproximation` below.\n",
    "\n",
    "*Hints: For $\\tau \\sim \\text{Gamma}(\\tau|a,b)$, the expected value is $\\mathbb{E}\\left[\\tau\\right] = \\frac{a}{b}$*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalApproximation(object):\n",
    "\n",
    "    def __init__(self, x, mu0=0, lambda0=0, a0=1, b0=1):\n",
    "        # store data & hyperparameters\n",
    "        self.x = x\n",
    "        self.N = len(x)\n",
    "        self.mu0 = mu0\n",
    "        self.lambda0 = lambda0\n",
    "        self.a0 = a0\n",
    "        self.b0 = b0\n",
    "\n",
    "    def initialize(self, m, v, a, b):\n",
    "        self.m = m\n",
    "        self.v = v\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        return self\n",
    "\n",
    "    def fit_approx(self, num_iterations):\n",
    "        \"\"\" Computes the optimal values for variational parameters (m, v, a,b) using CAVI byiteratively updating the components q(\\mu) = N(\\mu|m, v) and q(\\tau) = Gamma(\\tau|a,b) \"\"\"\n",
    "\n",
    "        # precompute \n",
    "        xbar = np.mean(self.x)\n",
    "        x2bar = np.mean(self.x**2)\n",
    "\n",
    "        # initialize parameters\n",
    "        m, v = self.m, self.v\n",
    "        a, b = self.a, self.b\n",
    "\n",
    "        for itt in range(num_iterations):\n",
    "\n",
    "            ##############################################\n",
    "            # Your solution goes here\n",
    "            ##############################################\n",
    "            ##############################################\n",
    "            # End of solution\n",
    "            ##############################################\n",
    "\n",
    "        # sanity check and store optimal variational parameters\n",
    "        assert v > 0, f\"The parameter v must be positive scalar, but the actual value for v was {v:4.3f}. Check your implementation.\"\n",
    "        assert a > 0, f\"The parameter a must be positive scalar, but the actual value for a was {a:4.3f}. Check your implementation.\"\n",
    "        assert b > 0, f\"The parameter b must be positive scalar, but the actual value for b was {b:4.3f}. Check your implementation.\"\n",
    "        self.a, self.b = a, b\n",
    "        self.m, self.v = m, v\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def pdf(self, mu, tau):\n",
    "        return log_npdf(mu, self.m, self.v).sum(axis=2) + log_gamma(tau, self.a, self.b).sum(axis=2)\n",
    "    \n",
    "# sanity check of results\n",
    "approx = VariationalApproximation(np.array([-1, 1]), mu0, lambda0, a0, b0).initialize(0, 1, 1, 1).fit_approx(100)\n",
    "assert np.allclose(np.array([approx.m, approx.v, approx.a, approx.b]), np.array([-0.33333333,  0.38888889,  2.5       ,  2.91666667])), 'Expected m, v, a, b. Please check your implementation.'\n",
    "\n",
    "# initialize approximation\n",
    "approx = VariationalApproximation(x, mu0, lambda0, a0, b0)\n",
    "approx.initialize(m=0.3, v=0.1, a=10, b=10)\n",
    "init_approx_grid = Grid2D(mus, taus, approx.pdf, name='Initial approximation')\n",
    "\n",
    "# optimize approximation\n",
    "approx.fit_approx(num_iterations=100)\n",
    "approx_grid = Grid2D(mus, taus, approx.pdf, name='Approximation')\n",
    "\n",
    "# visualize\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "exact_posterior_grid.plot_contours(ax[0], f=np.exp)\n",
    "exact_posterior_grid.plot_contours(ax[1], f=np.exp)\n",
    "init_approx_grid.plot_contours(ax[0], f=np.exp, color='r')\n",
    "approx_grid.plot_contours(ax[1], f=np.exp, color='r')\n",
    "ax[0].legend()\n",
    "ax[1].legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was implemented correctly, you should see a plot of the approximate posterior before (left panel) and after (right panel) the optimization process.\n",
    "\n",
    "**Task 1.6**: Write the integral for computing the approximate posterior predictive distribution $p(x^*|\\mathcal{D})$ using the variational approximation $q(\\mu, \\tau)$. You don't have to solve compute $p(x^*|\\mathcal{D})$, but just write up the integral. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7**: What is the approximate marginal distribution of $q(\\mu)$ and $q(\\tau)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Bayesian Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Gaussian mixture model** is very common in machine learning and statistics with a multitude of applications in clustering, density estimation, and outlier detection. It is also often used a building blocks in more complicated models.\n",
    "\n",
    "The Gaussian mixture model (GMM) for $D$-dimensional data is a weighted mixture of $K$ Gaussians:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{x}_n|\\pi, \\mathbf{m}, \\Lambda) = \\sum_{k=1}^K \\pi_k N(\\mathbf{x}_n|\\mathbf{m}_k, \\Lambda_k^{-1}), \\tag{1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{m}_k \\in \\mathbb{D}^D$ and $\\Lambda_k \\in \\mathbb{R}^{D \\times D}$ are the mean vector and precision matrix for the $k$'th component. The parameters $\\pi_k \\in [0, 1]$ for $k = 1, .., K$ are the **mixing weights** and satisfy $\\sum_{k=1}^K \\pi_k = 1$. Intuitively, $\\pi_k$ represents the fraction of samples belonging to the $k$'th component.\n",
    "\n",
    "**The GMM as a latent variable model**\n",
    "\n",
    "The GMM can also be formulated as a latent variable model with latent variables $z_n$ such that\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{x}_n|z_n, m, \\Lambda) = \\prod_{k=1}^K N(\\mathbf{x}_n|\\mathbf{m}_k, \\Lambda_k^{-1})^{z_{nk}} \\tag{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $z_n$ is a $K$-dimensional one-hot encoded binary vector indicating the component index for the $n$'th observation. This implies\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{x}_n|z_n=j, m, \\Lambda) =  N(\\mathbf{x}_n|m_j, \\Lambda_j^{-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "Note that we often abuse the notation and write e.g. $z_n = 3$ as a shorthand notation for $z_n = \\begin{bmatrix} 0 & 0 & 1 & 0 \\dots 0 \\end{bmatrix}$ and so on.\n",
    "The distribution of the latent variables is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(z_n|\\pi) = \\text{Categorial}(z_n|\\pi) = \\prod_{k=1}^K \\pi_k^{z_{nk}} \\tag{3},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\pi_k$ is the proportion of sample from the $k$'th component, i.e. $p(z_n=j|\\pi) = \\pi_j$. The variable $z_n$ is called a **latent variable** because we can't observe it, we can only observe $\\mathbf{x}_n$. \n",
    "\n",
    "We can derive the marginalized formulation in eq. (1) from the latent variable formulation in eq. (2)-(3) by applying to sum rule to marginalize over the latent variable $z_n$, i.e. $p(\\mathbf{x}_n) = \\sum_{k} p(\\mathbf{x}_n|z_n=k)p(z_n=k)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Maximum likelihood**\n",
    "\n",
    "The classic way to estimate the model parameters for GMMs ($m_k, \\Lambda_k, \\pi_k$ for $k = 1, .., K$) is through maximum likelihood estimation using the Expectation-Maximimization (EM) algorithm (not part of the curriculum of this course). However, the maximum likelihood solution is not well-posed and can suffer from singularities when a component \"collapses\" on a single data point. This causes the variance component for the collapsing component to go to zero and eventually the algorithm will crash.\n",
    "\n",
    "**Imposing priors**\n",
    "\n",
    "We can avoid these issues by using Bayesian inference to estimate the parameters. As usual, we impose prior distributions on the parameters: $m_k$, $\\Lambda_k$, and $\\pi_k$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\pi &\\sim \\text{Dirichlet}(\\alpha_0)\\\\\n",
    "%\n",
    "\\Lambda_k &\\sim \\text{Wishart}(W_0, \\nu_0)\\\\\n",
    "%\n",
    "\\mu_k|\\Lambda_k &\\sim \\text{Normal}(m_0, \\left(\\beta_0\\Lambda_k\\right)^{-1})\\\\\n",
    "%\n",
    "z_n|\\pi &\\sim \\text{Categorical}(\\pi)\\\\\n",
    "%\n",
    "x_n|\\mu, \\Lambda, z_n &\\sim \\text{Normal}(\\mu_{z_n}, \\Lambda_{z_n}^{-1}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we consider all parameters with subscript '0' as hyperparameters. \n",
    "\n",
    "\n",
    "**Task 2.1**: What is the sample space for each of the distributions above?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generative story**\n",
    "\n",
    "We can interret and visualize the assumptions of the model by considering the generative story of the model. Assume the number of observations $N$ and the number of components $K$ are fixed. By using Bayesian GMM to model a data set, we assume that the data set has been generated in the following way:\n",
    "\n",
    "1. Nature samples the mixing weights $\\pi\\sim \\text{Dirichlet}(\\alpha_0)$\n",
    "\n",
    "\n",
    "2. Nature samples a precision matrix $\\Lambda_k\\sim \\text{Wishart}(W_0, \\nu_0)$ for each component $k = 1, ..., K$\n",
    "\n",
    "\n",
    "3. Nature then samples a mean $m_k|\\Lambda_k\\sim \\text{Normal}(m_0, \\left(\\beta_0\\Lambda_k\\right)^{-1})$ for each component $k=1, ..., K$ conditioned on $\\Lambda_k$\n",
    "\n",
    "\n",
    "4. For each data point, nature samples a component index $z_n|\\pi \\sim \\text{Categorical}(\\pi)$\n",
    "\n",
    "\n",
    "5. For each data point, nature samples an observation from the relevant cluster distribution $x_n|\\mu, \\Lambda, z_n \\sim \\text{Normal}(\\mu_{z_n}, \\Lambda_{z_n}^{-1})$\n",
    "\n",
    "\n",
    "We can use **ancestral sampling** to generate synthetic data sets from this model\n",
    "\n",
    "**Task 2.2**: Complete the implementation of ancestral sampling below.\n",
    "\n",
    "*Hints: The functions `np.random.dirichlet`, `np.random.choice`, `scipy.stats.wishart.rvs` will be handy.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# number of components K,  dimension D, and number of observations N\n",
    "K = 6\n",
    "D = 2\n",
    "N = 1000\n",
    "\n",
    "# hyperparameter for the Dirichlet distribution\n",
    "alpha0 = 0.5*np.ones(K)\n",
    "\n",
    "# hyperparameters for the components\n",
    "m0 = np.zeros(D)\n",
    "W0 = np.identity(D)\n",
    "nu0 = 10\n",
    "beta0 = 0.1\n",
    "\n",
    "# Step 1: sample mixing weights (numpy array with shape (K,))\n",
    "pi = <insert code here>\n",
    "\n",
    "# Step 2: sample prior precision matrix for each component (numpy array with shape (K, D, D))\n",
    "Lambda_k = <insert code here>\n",
    "\n",
    "# Step 3: sample prior mean vector for each component\n",
    "Sk = np.zeros((K, D, D))\n",
    "mk = np.zeros((K, D))\n",
    "for k in range(K):\n",
    "    Sk[k, :, :] = np.linalg.inv(Lambda_k[k])\n",
    "    mk[k, :] = <insert code here>\n",
    "    \n",
    "# Step 4: sample latent variable for each observation (numpy array with shape (N, ))\n",
    "z = <insert code here>\n",
    "\n",
    "# Step 5: sample observations for all n (numpy array with shape (N, D))\n",
    "X = np.zeros((N, D))\n",
    "for n in range(N):\n",
    "    X[n] = <insert code here>\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# plot mixing weights\n",
    "axes[0,0].bar(np.arange(1,K+1), pi)\n",
    "axes[0,0].set(xlabel='Component index k', ylabel = '$\\pi_k$', xticks=np.arange(1, K+1))\n",
    "axes[0,0].set_title('Mixing weights $\\pi$')\n",
    "\n",
    "# plot component mean and covariances\n",
    "for k in range(K):\n",
    "    axes[0,1].plot(mk[k, 0], mk[k, 1], 'x', color=colors[k], markersize=15, label='k=%d' % (k+1)) \n",
    "    plot_std_dev_contour(axes[0, 1], mk[k], Sk[k], facecolor=colors[k], alpha=0.2)\n",
    "    plot_std_dev_contour(axes[1, 0], mk[k], Sk[k], facecolor=colors[k], alpha=0.2)\n",
    "    axes[1,0].plot(X[z==k, 0], X[z==k, 1], '.', color=colors[k], label='k=%d' % (k+1))\n",
    "\n",
    "axes[0,1].set_title('Component means and covariances ', fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[1,0].set_title('Joint distribution $p(x, z)$', fontweight='bold');\n",
    "axes[1,0].legend(markerscale=3)\n",
    "\n",
    "# plot observations\n",
    "axes[1,1].plot(X[:, 0], X[:, 1], '.', color='k')\n",
    "axes[1,1].set_title('Marginal distribution $p(x)$', fontweight='bold');\n",
    "    \n",
    "# add labels etc\n",
    "for a in [1, 2, 3]:\n",
    "    axes.flat[a].set(xlim = (-5, 5), ylim=(-5, 5), xlabel='$x_1$', ylabel='$x_2$')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: Why is the Dirichlet distribution sometimes called a \"distribution over distributions\"?  [**Discussion question**]\n",
    "\n",
    "\n",
    "**Task 2.4**: How does the hyperparameter $\\alpha_0$ affect the generated samples? Explore the generated datasets for different values of $\\alpha_0$, e.g. $\\alpha_0 = 0.2 \\cdot \\mathbf{1}$, $\\alpha_0 = 1 \\cdot \\mathbf{1}$, and $\\alpha_0 = 10 \\cdot \\mathbf{1}$, where $\\mathbf{1}$ is a $K$-dimensional vector of ones. Inspect the results for a few different random seeds. [**Discussion question**]\n",
    "\n",
    "\n",
    "**Task 2.5**: Relate the panels in the figure above to the \"generative story\" explained above. [**Discussion question**]\n",
    "\n",
    "**Task 2.6**: Which hyperparameter do we need to change in order to increase the separation between clusters, i.e. to increase the distance between the means of each cluster? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Variational inference for the Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to set-up the variational approximation for the mixture model. The joint distribution of observed data $\\mathbf{X}$, latent variable $\\mathbf{Z}$ and model parameters $\\mu, \\Lambda, \\pi$ follows from the **product rule**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(X, Z, \\mu, \\Lambda, \\pi) &=\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}(x_n|\\mu_k, \\Lambda^{-1}_k)^{z_{nk}}\\prod_{n=1}^N \\text{Cat}(z_n|\\pi)\\,\\text{Dir}(\\pi|\\alpha) \\prod_{k=1}^K p(\\mu_k, \\Lambda_k),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mu_k, \\Lambda_k) = p(\\mu_k|\\Lambda_k)p(\\Lambda_k) = \\mathcal{N}(\\mu_k|m_0, \\left[\\beta_0 \\Lambda_k\\right]^{-1})\\mathcal{W}(\\Lambda_k|W_0, \\nu_0).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Our goal is to compute the posterior distribution of the parameters conditioned on the data $X$, i.e. $p(Z, \\mu, \\Lambda, \\pi|X)$. However, computing the exact posterior distribution for this model would require us to sum over all $K^N$ possible configurations of the latent variables $z_n$, which becomes infeasible for even small to moderate sized datasets. Therefore, we have to resort to approximate inference. In this exercise, we will work with **variational approximations**, but one could also implement an MH-sampler or derive a Gibbs sampler for the model.\n",
    "\n",
    "Informally, we define a collection of tractable distributions $\\mathcal{Q}$ and search for the distribution $q \\in \\mathcal{Q}$ that resembles the true posterior distribution as close as possible as measured by the **Kullback-Leibler (KL) divergence** $\\text{KL}[q||p]$. \n",
    "\n",
    "For the Baysian GMM, we choose the variational family $\\mathcal{Q}$ to be the collection of all distributions with following factorization\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(Z, \\mu, \\Lambda, \\pi) = q(Z)q(\\mu, \\Lambda, \\pi)\n",
    "\\end{align*}$$\n",
    "\n",
    "That is, we assume that the latent variables $z_n$ are independent from the rest of variables and then we search for the best matching distribution under this independence assumption. Note that this is the only assumption we need in order to be able to derive a variational approximation for the posterior.\n",
    "\n",
    "To identify the best matching distribution $q^* \\in \\mathcal{Q}$, we minimize the KL-divergence between $q \\in \\mathcal{Q}$ and $p \\equiv p(Z, \\mu, \\Lambda, \\pi|X)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "q^* = \\arg\\min\\limits_{q \\in \\mathcal{Q}} \\text{KL}[q||p]. \\tag{4}\n",
    "\\end{align*}$$\n",
    "\n",
    "As shown in the lecture, the KL-divergence $\\text{KL}[q||p]$ can we re-written as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{KL}[q||p] = \\ln p(X) - \\mathcal{L}[q] \\quad\\quad\\iff\\quad\\quad \\ln p(X) = \\text{KL}[q||p] + \\mathcal{L}[q], \\tag{5}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $p(X)$ is the marginal likelihood and the $\\mathcal{L}[q]$ is the **evidence lowerbound (ELBO)** defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}[q] = \\mathbb{E}_q\\left[\\ln p(X, Z, \\mu, \\Lambda, \\pi)\\right] - \\mathbb{E}_q\\left[\\ln q(Z, \\mu, \\Lambda, \\pi)\\right] \\tag{6}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "In practice, we optimize the **lower bound** $\\mathcal{L}[q]$ rather than the KL divergence\n",
    "\n",
    "$$\\begin{align*}\n",
    "q^* = \\arg\\max\\limits_{q \\in \\mathcal{Q}} \\mathcal{L}[q]. \\tag{7}\n",
    "\\end{align*}$$\n",
    "\n",
    "Using the CAVI algorithm leads to iterating the following two steps:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\ln q(Z) &\\propto \\mathbb{E}_{q(\\mu, \\Lambda, \\pi)}\\left[\\ln p(X, Z, \\mu, \\Lambda, \\pi)\\right] \\tag{8}\\\\\n",
    "\\ln q(\\pi, \\mu, \\Lambda) &\\propto \\mathbb{E}_{q(Z)}\\left[\\ln p(X, Z, \\mu, \\Lambda, \\pi)\\right]. \\tag{9}\n",
    "\\end{align*}$$\n",
    "\n",
    "We fit the approximation by optimizing the lower bound $\\mathcal{L}[q]$. Therefore, we can evaluate and monitor $\\mathcal{L}[q]$ using eq. (6) to check when the optimization has converged.\n",
    "\n",
    "Eq. (8) and (9) above can be reduced to a set of simple update equations as done in Bishop from eq. (10.43) to (10.68). In this course, we won't dive into these specific calculations, instead we will focus on understanding the general methodology. That is, you should be able to understand and explain the ideas behind equations eq. (4)-(9) from a conceptual point of view, but the steps from eq. (8)-(9) to eq. (10) will not be part of the curriculum.\n",
    "\n",
    "Don't worry if these concepts appear very abstract now, we will see more examples in the following two weeks, but make sure the discuss the key equations with one of the teachers, if you find them confusing.\n",
    "\n",
    "The optimal variational approximation takes the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(Z, \\pi, \\mu, \\Lambda) = q(Z)q(\\pi)q(\\mu, \\Lambda) = \\underbrace{\\prod_{n=1}^N \\text{Categorial}(z_n|r_n)}_{q(Z)} \\underbrace{\\text{Dir}(\\pi|\\alpha)}_{q(\\pi)}\\underbrace{\\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k|m_k, \\left[\\beta_k\\Lambda_k\\right]^{-1}\\right)\\mathcal{W}(\\Lambda_k|W_k, \\nu_k)}_{q(\\mu, \\Lambda)} \\tag{10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $Z$ represents the cluster assignments, and the (approximate) posterior distribution $Q(Z)$ represents the distribution over cluster assignments for each data point after having seen the data. The posterior cluster assignment for the $n$'th datapoint is $q(z_n) = \\text{Categorical}(z_n|r_n)$, where $z_n$ is cluster index (and sometimes represented as a one-hot encoded vector) and $r_n$ is a vector probabilities such that probability of the $n$'th data point being in cluster $k$ is $r_{n,k}$. These probabilities are often called ***responsibilities*** as the number $r_{n,k}$ can be interpreted as how much the $k$'th cluster is responsible for explaning the $n$'th data point. \n",
    "\n",
    "We often defined the **effective number of data points** in the $k$'th cluster to be\n",
    "\n",
    "\\begin{align*}\n",
    "N_k = \\sum_{n=1}^N r_{n, k} \\tag{eq. (10.51) in Bishop}\n",
    "\\end{align*}\n",
    "\n",
    "The posterior distribution of the mixing weights is $Q(\\pi) = \\text{Dir}(\\pi|\\alpha)$, where $\\alpha \\in \\mathbb{R}^D_+$ is as vector of parameters for the Dirichlet distribution, where $a_k = a_0 + N_k$. The posterior mean of the $k$'th mixing weight is therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[\\pi_k|X\\right] = \\frac{\\alpha_0 + N_k}{K\\alpha_0 + N} \\tag{eq. (10.69) in Bishop}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that there is a typo in the corresponding equation in Bishop eq. (10.69). \n",
    "The posterior distribution for the $k$'th cluster mean and cluster precision is $q(\\mu_k|\\Lambda_k) = \\mathcal{N}(\\mu_k|m_k, \\left[\\beta_k \\Lambda_k\\right]^{-1})$ and $q(\\Lambda_k) = \\mathcal{W}(W_k, \\nu_k)$, respectively, where the parameters are given by\n",
    "\n",
    "\\begin{align*}\n",
    "    \\beta_k &= \\beta_0 + N_k\\\\\n",
    "    m_k &= \\frac{1}{\\beta_k}(\\beta_0 m_0 + N_k \\bar{x}_k)\\tag{10b}\\\\\n",
    "    W_k^{-1} &= W^{-1}_0 + N_k S_k + \\frac{\\beta_0 N_k}{\\beta_0 + N_k} (\\bar{x}_k - m_0)(\\bar{x}_k-m_0)^T\\\\\n",
    "    \\nu_k &= N_k + 1\n",
    "\\end{align*}\n",
    "\n",
    "for \n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{x}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{n,k} x_n\\\\\n",
    "S_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{n,k} (x_n - \\bar{x}_n)(x_n - \\bar{x}_n)^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 3.1**: Use eq. (5) to explain why $\\mathcal{L}[q]$ is a lower bound on $\\ln p(X)$. [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 3.2**: Use eq. (5) to explain why maximizing the lower bound $\\mathcal{L}[q]$ is equivalent to minimizing the KL-divergence wrt. $q$ [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3**: Show that the posterior mean $m_k$ for each cluster  is a convex combination of the prior mean and the weighed average of the data points $\\bar{x}_k$. That is, show that there exist some value $\\rho \\in \\left[0, 1\\right]$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    m_k = (1-\\rho)m_0 + \\rho \\bar{x}_k\n",
    "\\end{align*}\n",
    "\n",
    "*Hints*: \n",
    "- Argue that $\\rho = \\frac{N_k}{\\beta_k} \\in \\left[0, 1\\right]$ and $\\frac{\\beta_0}{\\beta_k} = 1 - \\rho$.\n",
    "- If you don't know how to get started here, do not hesitate to ask for help :-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4**: **Optional**  Show that the posterior mean for cluster $k$ converges to the prior mean when $\\beta_0 \\rightarrow \\infty$ using eq. (10.b)\n",
    "\n",
    "*Hints*\n",
    "- What happens to $\\beta_k$ when $\\beta_0 \\rightarrow \\infty$?\n",
    "- If you don't know how to get started here, do not hesitate to ask for help :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Analyzing a toy data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `VariationalGMM` in the module `exercise10.py` provides a basic implementation of variational approximation of the GMM described above.\n",
    "First, we will use the Variational GMM algorithm to analyze a simple toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('./exercise10_toydata.npz')\n",
    "X = data['X']\n",
    "N = len(X)\n",
    "\n",
    "# split data in training/test\n",
    "np.random.seed(0)\n",
    "Ntrain = int(0.5*N)\n",
    "Ntest = N - Ntrain\n",
    "np.random.shuffle(X)\n",
    "Xtrain, Xtest = X[:Ntrain], X[Ntrain:]\n",
    "\n",
    "# plot data\n",
    "def plot_data(ax=None, train=True, test=True, legend=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    if train:\n",
    "        ax.plot(Xtrain[:, 0], Xtrain[:, 1], 'k.', label='Training', markersize=9)\n",
    "    if test:\n",
    "        ax.plot(Xtest[:, 0], Xtest[:, 1], 'g.', label='Test')\n",
    "    if legend:\n",
    "        ax.legend(markerscale=1.5)\n",
    "    ax.set(xlabel='$x_1$', ylabel='$x_2$', xlim=(-10, 10), ylim=(-10, 10))\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection of the data, there seems to be three clusters. So let's run the variational algorithm for $K = 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with dimension D = 2 and K = 3 number of clusters.\n",
    "vi = VariationalGMM(D=2, K=3, alpha0=.1, seed=0)   \n",
    "vi.fit(Xtrain, max_itt=10000)\n",
    "\n",
    "# compute probabilities p(z_n|x_n) (for the training data, we could also have used vi.r instead of ptrain)\n",
    "ptrain = vi.compute_component_probs(Xtrain)\n",
    "ztrain = np.argmax(ptrain, axis=1)\n",
    "\n",
    "# prepare grid for log predictive density (lpd)\n",
    "P = 100\n",
    "x_grid = np.linspace(-10, 10, P)\n",
    "XX, YY = np.meshgrid(x_grid, x_grid)\n",
    "Xp = np.column_stack((XX.ravel(), YY.ravel()))\n",
    "lpd = vi.evaulate_log_predictive(Xp, pointwise=True).reshape((P, P))\n",
    "\n",
    "# evaluate log predictive density for test set\n",
    "test_lpd = vi.evaulate_log_predictive(Xtest)\n",
    "print(f'Test LPD for K={vi.K}: {test_lpd:4.3f}')\n",
    "\n",
    "# prep. plot\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 6))\n",
    "# plot mixing weights\n",
    "barlist = axes[0].bar(np.arange(1, vi.K+1), vi.pi)\n",
    "\n",
    "for k in range(vi.K):\n",
    "    barlist[k].set_color(colors[k])\n",
    "    \n",
    "# plot data, clusters and assignments\n",
    "plot_data(ax=axes[1], test=False, legend=False)\n",
    "handles, labels = [], []\n",
    "for k in range(vi.K):\n",
    "    plot_std_dev_contour(axes[1], vi.m[k], vi.S[k], facecolor=colors[k], alpha=vi.pi[k]/np.max(vi.pi), label='k =%d' % (k+1))\n",
    "    hk = axes[2].plot(Xtrain[ztrain==k, 0], Xtrain[ztrain==k, 1], '.', color=colors[k])\n",
    "    handles.append(hk[0])\n",
    "    labels.append('k = %d' % (k+1))\n",
    "\n",
    "# plot predictive density\n",
    "axes[3].pcolormesh(x_grid, x_grid, np.exp(lpd), cmap=plt.cm.RdBu_r, shading='auto')\n",
    "\n",
    "    \n",
    "# labels, titles etc\n",
    "axes[0].set(title='Posterior mean of mixture weights', xlabel='Component index $k$');\n",
    "axes[1].set(title='Estimated components', xlabel='$x_1$', ylabel='$x_2$')\n",
    "axes[2].set(title='Component assignments', xlim=(-10, 10), ylim=(-10, 10), xlabel='$x_1$', ylabel='$x_2$')\n",
    "axes[3].set(xlabel='$x_1$', title='Log predictive density')\n",
    "\n",
    "fig.legend(handles, labels, loc='center right', markerscale=3);\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Explain what you see in the three panels. What is the value of the lower bound for $K = 3$? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.2**: What happens if you run the analysis with $K = 10$? Explain what you see. What is the value of the lower bound for $K = 10$? [**Discussion question**]\n",
    "\n",
    "*Notes*\n",
    "- the algorithm may take a bit longer to run for $K = 10$  \n",
    "- the plotting code does not work for $K \\geq 11$ due to the chosen color scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 4.3**: In the simulation, we have used a Dirichlet prior with hyperparameter value $\\alpha_0 = 10$. What happens of you decrease it to $\\alpha_0 = 0.1$? What is the value of the lower bound? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.4**: What is the interpretation of the $\\alpha_0$ parameter and how does it affect the prior distribution of the mixing weights $\\pi$? Use eq. (10.69) in Bishop\n",
    " above to explain your reasoning. [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.5**: What happens if you run the analysis multiple times? Why does the assigned clusters change from run to run, but not the predictive density? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.6**: Only one of the following to questions is meaningful to answer. Which one and why?\n",
    "\n",
    "- What is the posterior probability of the $7$'th data point belonging to the $2$ nd cluster?\n",
    "- What is the posterior probability of the $7$'th and $9$'th data points beloging to the same cluster?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection**\n",
    "\n",
    "We can choose an optimal value for $K$ by measuring the log predictive density $p(x^*|\\bm{X})$ using eq. (10.80) in Bishop for an independent test/validation set as done below. First, let's repeat and plot the above figure for $K \\in \\left[1, 2, 3, 4, 5, 10\\right]$ for $\\alpha_0= 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models for K = 1, .., 5, 10\n",
    "Ks = [1, 2, 3, 4, 5, 10]\n",
    "fits = [VariationalGMM(D=2, K=K, alpha0=0.1).fit(Xtrain, max_itt=10000) for K in Ks]\n",
    "\n",
    "# prepare grid\n",
    "P = 100\n",
    "x_grid = np.linspace(-10, 10, P)\n",
    "XX, YY = np.meshgrid(x_grid, x_grid)\n",
    "Xp = np.column_stack((XX.ravel(), YY.ravel()))\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(2, len(fits), figsize=(25, 12))\n",
    "for idx_fit, fit in enumerate(fits):\n",
    "    \n",
    "    # compute class assignments for test set\n",
    "    z = np.argmax(fit.compute_component_probs(Xtest), axis=1)\n",
    "\n",
    "    # compute lpd\n",
    "    lpd = fit.evaulate_log_predictive(Xp, pointwise=True).reshape((P, P))\n",
    "\n",
    "    # plot\n",
    "    for k in range(fit.K):\n",
    "        plot_std_dev_contour(axes[0, idx_fit], fit.m[k], fit.S[k], facecolor=colors[k], alpha=0.5*fit.pi[k]/np.max(fit.pi), label='k =%d' % (k+1))\n",
    "        axes[0, idx_fit].plot(Xtest[z==k, 0], Xtest[z==k, 1], '.', color=colors[k])\n",
    "        axes[0, idx_fit].set(title='K = %d' % fit.K, ylim=(-12, 12), xlim=(-12, 12))\n",
    "        axes[1, idx_fit].pcolormesh(x_grid, x_grid, np.exp(lpd), cmap=plt.cm.RdBu_r, shading='auto')\n",
    "        axes[1, idx_fit].set(xlabel='$x_1$', title='$p(x^*|X)$ for $K = %d$' % fit.K)\n",
    "    axes[0, 0].set_ylabel('$x_2$')\n",
    "    axes[1, 0].set_ylabel('$x_2$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.7**: Compute the average log predictive density for the test for each K and plot it as a function of $K$. What is the optimal number of clusters?\n",
    "\n",
    "*Hint: use the function `evalulate_log_predictive` from the `VariationalGMMÂ´ object*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Clustering images using the Bayesian Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to apply the Bayesian mixture model to cluster images. We will work with the same data as in exercise 6. That is, a subset of images from the Linnaeus 5 dataset (http://chaladze.com/l5/), where we have use a ResNet18 network as feature extractor. See Exercise 8 for more details. The main difference is now that we wont feed the image labels to the algorithm, but only the image features.\n",
    "\n",
    "First, we load the data and reduce to the dimensionity using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./ex7_data.npz')\n",
    "labels = list(data['labels'])\n",
    "features = data['features']\n",
    "targets = data['targets']\n",
    "num_classes = data['num_classes'][()]\n",
    "\n",
    "X = PCA_dim_reduction(features, num_components=10)\n",
    "N, D = X.shape\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "Ntrain = int(0.8*N)\n",
    "Ntest = N - Ntrain\n",
    "\n",
    "idx_train = np.random.choice(range(N), size=Ntrain, replace=False)\n",
    "idx_test = np.setdiff1d(range(N), idx_train)\n",
    "\n",
    "print('Number of images: %d' % N)\n",
    "print('Number of features: %d' % D)\n",
    "\n",
    "Xtrain, Xtest = X[idx_train, :], X[idx_test, :]\n",
    "ttrain, ttest = targets[idx_train], targets[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(ax, i, title=True):\n",
    "    img = Image.open('./images/%d.jpg' % i)\n",
    "    target = targets[i]\n",
    "    label = labels[int(target)]\n",
    "    ax.imshow(img)\n",
    "    if title:\n",
    "        ax.set_title(label)\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, ax_ = plt.subplots(2, 5, figsize=(20, 6))\n",
    "ax = ax_.flat\n",
    "for i in range(10):\n",
    "    show_example(ax[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the images come from a dataset containing the following four labels: berry, flower, dog, bird. \n",
    "Let's fit a Bayesian GMM with $K = 4$ to the training set and inspect images from the identified components. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "vi = VariationalGMM(D=D, K=4, alpha0=1e-3).fit(Xtrain, max_itt=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior mean of the mixing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\n",
    "ax.bar(np.arange(1, vi.K+1), vi.pi)\n",
    "ax.set(xlabel='Component index', ylabel='Posterior mean for $\\pi_k$', title='Posterior mean of mixing weights', xticks=np.linspace(1, vi.K, min(vi.K, 20)).round());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes images from some of the larger clusters (containing 30 images or more)\n",
    "\n",
    "**Task 4.8**: Does the four clusters align with the target labels of the data set? [**Discussion question**]\n",
    "\n",
    "**Task 4.9**: Increase the number of clusters to $K = 50$ and repeat the analysis. Can you spot any patterns representative for each cluster?  [**Discussion question**]\n",
    "\n",
    "- Note: We allow for a maximum of 10000 iterations, but it should converge much earlier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img_clustering(z, K, idx_list):\n",
    "    \n",
    "    for k in range(K):\n",
    "        \n",
    "        # find and count all images assigned to cluster k\n",
    "        idx_k = np.where(z == k)[0]\n",
    "        \n",
    "        num_images_in_cluster = len(idx_k)\n",
    "        \n",
    "        if num_images_in_cluster < 30:\n",
    "            continue\n",
    "\n",
    "        # visualiz\n",
    "        fig, axes = plt.subplots(3, 5, figsize=(20, 12.5))\n",
    "\n",
    "        for idx_plot, idx in enumerate(idx_k):\n",
    "            if idx_plot >= np.prod(axes.shape):\n",
    "                break\n",
    "\n",
    "            show_example(axes.flat[idx_plot], idx_list[idx], False)\n",
    "\n",
    "        fig.subplots_adjust(hspace=0.0, wspace=0.00)\n",
    "        fig.suptitle('Cluster idx %d with %d images' % (k+1, num_images_in_cluster), fontweight='bold')\n",
    "\n",
    "\n",
    "ptrain = vi.compute_component_probs(Xtrain)\n",
    "ztrain = np.argmax(ptrain, axis=1)\n",
    "visualize_img_clustering(ztrain, vi.K, idx_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
