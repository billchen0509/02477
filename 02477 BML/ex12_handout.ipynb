{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Exercise 12: Black-box variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import autograd.numpy as np\n",
    "import seaborn as snb\n",
    "\n",
    "from time import time\n",
    "\n",
    "from autograd import grad\n",
    "from autograd.scipy.stats import norm\n",
    "\n",
    "from exercise12 import create_linear_regression_data\n",
    "from exercise12 import AdamOptimizer\n",
    "\n",
    "# relevant distributions\n",
    "npdf = lambda x, m, v: np.exp(-(x-m)**2/(2*v))/np.sqrt(2*np.pi*v)\n",
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*np.log(2*np.pi*v)\n",
    "\n",
    "# plotting stuff\n",
    "snb.set_style('darkgrid')\n",
    "snb.set_theme(font_scale=1.25)\n",
    "colors = snb.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to study and become familiar with **black-box variational inference (BBVI)**, which combines a large proportion of the theory we discussed through the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content**\n",
    "\n",
    "- Part 1: Kullback-leibler divergences and mean-field variational families\n",
    "- Part 2: Black-box variational inference theory\n",
    "- Part 3: Variational Inference for a linear Gaussian model\n",
    "- Part 4: Black-box variational inference (BBVI) for the Linear Gaussian model\n",
    "- Part 5: Spam vs ham detection\n",
    "- Part 6: Prototyping and testing models using BBVI\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n",
    "**Note**: If you find yourself spending more than 30minutes on a single task, consider asking for help or look up at the solution for inspiration and move forward.\n",
    "\n",
    "\n",
    "In part 1, we will the combination of mean-field families with the  KL[q||p]  divergence. In part 2, we will review the basic theory of BBVI. To understand the properties of BBVI, we will compare BBVI to classic VI using a linear Gaussian system. Part 3 deals with the classic variational approximation to the linear Gaussian system and part 4 deals with BBVI counterpart. Finally, in part 5+6 we will build a simple text classifier for SPAM detection. We will see how to use BBVI to rapidly comparing and testing different models for binary classification.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  Kullback-leibler divergences and mean-field variational families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the black-box variational inference algorithm, we will first study the combination of variational inference using $\\text{KL}[q||p]$ with mean-field Gaussian families. For the purpose of this analysis, we will assume our target distribution $p$ is a multivariate Gaussian distribution\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{w}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{V}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^D$ and $\\mathbf{m} \\in \\mathbb{R}^D$ is the mean vector and $\\mathbf{V} \\in \\mathbb{R}^{D \\times D}$ covariance matrix. Assuming a mean-field Gaussian family, the variational family $\\mathcal{Q}$ is then the collection of distributions of the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(\\mathbf{w}) = \\prod_{i=1}^D \\mathcal{N}(\\mathbf{w}|m_i, v_i) = \\mathcal{N}(\\mathbf{w}|\\hat{\\mathbf{m}}, \\hat{\\mathbf{V}}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\hat{\\mathbf{V}}$ is a **diagonal matrix** with elements $\\hat{\\mathbf{V}}_{ii} = v_i > 0$ and $\\hat{\\mathbf{V}}_{ij} = 0$ for $i \\neq j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: Does the target distribution $p(\\mathbf{w}|\\mathbf{y})$ belong to the variational family? Why/why not?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now study the optimal approximation $q^*$ in this set-up. More specifically, we are interested in the qualitative behavior of the estimated variances for $q^*$ given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "q^* = \\arg\\min_{q\\in \\mathcal{Q}}\\text{KL}[q(\\mathbf{w})||p(\\mathbf{w}|\\mathbf{y})]\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\text{KL}[q(\\mathbf{w})||p(\\mathbf{w}|\\mathbf{y})]$-divergence, which can be computed analytically for multivariate Gaussians:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{KL}[q||p] = \\frac12 \\left[\\text{trace}\\left(\\mathbf{V}^{-1}\\hat{\\mathbf{V}}\\right) + \\left(\\textbf{m}- \\hat{\\textbf{m}}\\right)^T\\mathbf{V}^{-1} \\left(\\textbf{m}- \\hat{\\textbf{m}}\\right) - D  + \\log \\frac{|\\mathbf{V}^{-1}|}{|\\hat{\\mathbf{V}}|}\\right],\n",
    "\\end{align*}$$\n",
    "\n",
    "where $D > 0$ is the dimension of $\\mathbf{w}$.\n",
    "\n",
    "Inspecting the equation above, we can see that the solution for the variational mean is given by $\\hat{\\mathbf{m}} = \\mathbf{m}$. The focus of this exercise is to study the variances and therefore, we will simplify the problem by assuming $\\hat{\\textbf{m}} = \\textbf{m} = 0$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{KL}[q(\\mathbf{w})||p(\\mathbf{w}|\\mathbf{y})] = \\frac12 \\left[\\text{trace}\\left(\\mathbf{V}^{-1}\\hat{\\mathbf{V}}\\right) - D  + \\log \\frac{|\\mathbf{V}^{-1}|}{|\\hat{\\mathbf{V}}|}\\right] \\tag{A}.\n",
    "\\end{align*}$$\n",
    "\n",
    "We don't need this assumption to carry out our analysis, but it is merely to limit the required work for the exercise. Next, we will compute the matrix derivative of eq. (A) with respect to $\\hat{\\mathbf{V}}$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, you will need the following results from matrix calculus (see e.g [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf) or Section 7.8  in Murphy1)\n",
    "\n",
    "- For matrix $\\mathbf{A}$ and $\\mathbf{D}$, where $\\mathbf{D}$ is diagonal, we have $\\frac{\\partial \\text{trace}(\\mathbf{A}\\mathbf{D})}{\\partial \\mathbf{D}_{ii}} = \\mathbf{A}_{ii}$ \n",
    "\n",
    "\n",
    "- The derivative of the log determinant of a **diagonal matrix** $\\mathbf{D}$ is given by $\\frac{\\partial \\log |\\mathbf{D}|}{\\partial \\mathbf{D}_{ii}} = \\frac{1}{\\mathbf{D}_{ii}}$ (Bonus exercise: can you prove this?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 1.2**: Show that the optimal value for $\\hat{\\mathbf{V}}_{ii}$ in this set-up is given by $\\hat{\\mathbf{V}}_{ii} = \\frac{1}{(\\mathbf{V}^{-1})_{ii} }$\n",
    "\n",
    "*Hints:  Compute the derivative of eq. (A), equate it to zero and solve for $\\hat{\\mathbf{V}}_{ii}$.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, assume $\\mathbf{V} = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1\\end{bmatrix}$ for $\\rho \\in \\left(-1, 1\\right)$ such that $\\rho$ controls the correlation of the target distribution.\n",
    "\n",
    "**Task 1.3**: Compute and plot  $\\mathbf{V}_{11}$ and $\\hat{\\textbf{V}}_{11}$ as a function of the correlation coefficient $\\rho \\in \\left(-1, 1\\right)$. How well does the mean-field approximation capture the correlation of the target distribution? How well does the mean-field approximation capture the marginal variances of the target distribution?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Black-box variational inference theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the exercise, we will focus on the **black-box variational inference (BBVI)** algorithm. Recall, the goal of variational inference is to find the best approximation $q \\in \\mathcal{Q}$ in a given variational family $\\mathcal{Q}$ by minimizing the KL divergence $\\text{KL}[q||p]$ between the approximation $q$ and the target distribution $p$. In practice, we do this by maximizing the so-called evidence lower bound (ELBO) given by\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}\\left[q\\right] = \\mathbb{E}_{q(\\mathbf{w})}\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right] - \\mathbb{E}_{q(\\mathbf{w})}\\left[\\log q(\\mathbf{w})\\right]\\tag{1},\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $p(\\mathbf{y}, \\mathbf{w}) = p(\\mathbf{y}|\\mathbf{w})p(\\mathbf{w})$ is the joint distribution of the observations $\\mathbf{y}$ and the parameters $\\mathbf{w}$.\n",
    "\n",
    "In this exercise, we will use the set of **mean-field Gaussians** as the variational family, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "q(\\mathbf{w}) = \\prod_{i=1}^D \\mathcal{N}(w_i|m_i, v_i) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{V}), \\tag{2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $\\mathbf{V}$ is a diagonal matrix and $\\lambda = \\left\\lbrace \\mathbf{m}, \\mathbf{V}\\right\\rbrace$ are the **variational parameters**. With this choice of variational family, the **entropy term** in eq. (1) can be computed analytically (which you will derive in assignment #3)\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{H}\\left[q\\right] = -\\mathbb{E}_{q(\\mathbf{w})}\\left[\\log q(\\mathbf{w})\\right] = \\frac12\\sum_{i=1}^D \\log (2\\text{e}\\pi v_i). \\tag{3}\n",
    "\\end{align*}\n",
    "\n",
    "In classic variational inference, we compute the expectation of the log joint distribution in eq. (1) analytically. However, not all distributions exhibit analytical solutions to these expectations and therefore, this limits the set of distributions we can use. Furthermore, every time we change a component of the model, we have to re-derive these expectations, which makes prototyping many different models a very time-consuming process. \n",
    "\n",
    "In BBVI, the expectation of the log joint density is estimated using Monte Carlo sampling. That is,\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q(\\mathbf{w})}\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right] \\approx \\frac{1}{S}\\sum_{s=1}^S \\log p(\\mathbf{y}, \\mathbf{w}^s), \\tag{4}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $\\mathbf{w}^s \\sim q(\\mathbf{w})$ are samples from the approximate posterior $q$ and $S$ is the number of Monte Carlo samples. Consequently, we can apply VI to a much larger set of models and distributions and we no longer have derive the expected values by hand everytime we want to test a new model. The price we pay for this flexibility is that our objective function and its gradients becomes **stochastic**.\n",
    "\n",
    "In this exercise, we will focus on the broad class of models, where the observations $\\mathbf{y}$ can be assumed **conditionally independent** given $\\mathbf{w}$. That is,\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = \\prod_{n=1}^N p(y_n|\\mathbf{w})p(\\mathbf{w}) \\tag{5}\n",
    "\\end{align*}\n",
    "\n",
    "This class of models includes linear regression, logistic regression, multiclass classification, neural networks, Gaussian processes etc. Substituting this decomposition into eq. (4) yields\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q(\\mathbf{w})}\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right] \\approx \\frac{1}{S}\\sum_{s=1}^S \\sum_{n=1}^N \\log  p(y_n|\\mathbf{w}^s) + \\frac{1}{S}\\sum_{s=1}^S \\log p(\\mathbf{w}^s) \\tag{6}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For large datasets, this can be prohibitively slow since we need to evaluate $N$ expectations in every single iteration. To alleviate this, we will use **minibatching** and further approximate eq. (6) as follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q(\\mathbf{w})}\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right]  \\approx \\frac{1}{S}\\frac{N}{M}\\sum_{s=1}^S \\sum_{n \\in \\mathcal{M}} \\log  p(y_n|\\mathbf{w}^s) + \\frac{1}{S}\\sum_{s=1}^S \\log p(\\mathbf{w}^s), \\tag{7}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes a random subset of the data with **batch size** $M = |\\mathcal{M}|$. Using this approximation, we only have to evaluate $M \\ll N$ terms in each iteration and if we resample $\\mathcal{M}$ in every iteration, then eq. (7) is an **unbiased** estimator of $\\mathbb{E}_{q(\\mathbf{w})}\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right]$.\n",
    "\n",
    "In this exercise, we will use the **re-parametrization trick** for estimating the gradients of the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\mathbf{\\lambda}} \\mathbb{E}_{q_{\\mathbf{\\lambda}}(\\mathbf{w})} \\! \\left[    \\log  p(\\mathbf{y},\\mathbf{w})\\right] \\approx \\frac{1}{S} \\sum_{s=1}^S  \\nabla_{\\mathbf{\\lambda}}  \\log  p(\\mathbf{y}, \\mathbf{w}_{\\epsilon_s}),\n",
    "\\end{align*}$$\n",
    "where $\\mathbf{w}_{\\epsilon_s} = \\mathbf{m} + \\mathbf{L}\\mathbf{\\epsilon}_{s}$ and $\\mathbf{\\epsilon}_{s} \\sim q(\\mathbf{\\epsilon})=\\mathcal{N}(\\mathbf{\\epsilon}|\\mathbf{0}, \\mathbf{I})$. Because of the mean-field assumption and the diagonal nature of $\\mathbf{V}$, we do not need to bother computing the Cholesky decomposition $\\mathbf{L}$ for $\\mathbf{V}$. Instead, let $\\mathbf{s}\\in \\mathbb{R}^D$ contain the standard deviations of the variational approximation, i.e. $\\mathbf{s}_i = \\mathbf{v}_i^{\\frac12}$, then\n",
    "$$\\begin{align*}\n",
    "\\mathbf{w}_{\\epsilon_s} = \\mathbf{m} + \\mathbf{s} \\circ \\epsilon_s,\n",
    "\\end{align*}$$\n",
    "where $\\circ$ means element-wise multiplication. Finally, we will use the **Adam** optimizer for optimization.\n",
    "\n",
    "**Task 2.1**: Explain to your neighbor why we need the re-parametrization trick in the first place, i.e. what is the problem with computing $\\nabla_{\\mathbf{\\lambda}} \\mathbb{E}_{q_{\\mathbf{\\lambda}}(\\mathbf{w})} \\! \\left[    \\log  p(\\mathbf{y},\\mathbf{w})\\right]$ directly? [**Discussion question**]\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Variational Inference for a linear Gaussian model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the behaviour of BBVI, we will first study the posterior distribution of a linear Gaussian model, because this allows us to:\n",
    "\n",
    "1) compute the exact posterior (using theory from week 3)\n",
    "2) compute the variational approximation using analytical ELBO (using theory from week 10 & 11)\n",
    "3) compute the variational approximation using the BBVI ELBO\n",
    "\n",
    "and compare the results. \n",
    "\n",
    "For a linear Gaussian model with observations $\\mathbf{y}$, design matrix $\\mathbf{X}$, and parameters $\\mathbf{w}$, the joint distribution of the model is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{X}\\mathbf{w}, \\sigma^2 \\mathbf{I})\\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\kappa^2 \\mathbf{I}), \\tag{8}.\n",
    "\\end{align}\n",
    "\n",
    "where the **exact posterior** for $\\mathbf{w}$ given $\\mathbf{y}$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}|\\mathbf{y}) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{m}, \\mathbf{\\Sigma}\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\Sigma}^{-1} &= \\frac{1}{\\kappa^2} \\mathbf{I} + \\frac{1}{\\sigma^2} \\mathbf{X}^T \\mathbf{X}\\\\\n",
    "\\mathbf{m} &= \\frac{1}{\\sigma^2} \\mathbf{\\Sigma} \\mathbf{X} \\mathbf{y}.\n",
    "\\end{align*}\n",
    "\n",
    "Hence, this will be our gold standard for comparison when using BBVI.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the analytical ELBO\n",
    "\n",
    " Before setting up the BBVI approximation, we will first derive an analytical **fixed-form** variational approximation for comparison. To do this, we have to calculate the lower bound. We only have to compute the first term in eq. (1) since we already know the entropy term. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting eq. (8) into the first term in eq. (1) yields\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}_q\\left[\\log p(\\mathbf{y}, \\mathbf{w})\\right] &= \\mathbb{E}_q\\left[\\log \\prod_{n=1}^N \\mathcal{N}(y_n|\\mathbf{w}^T\\mathbf{x}_n, \\sigma^2) \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\kappa^2 \\mathbf{I})\\right]\n",
    "%\n",
    "= \\sum_{n=1}^N \\mathbb{E}_q\\left[\\log  \\mathcal{N}(y_n|\\mathbf{w}^T\\mathbf{x}_n, \\sigma^2)\\right]+\n",
    "\\mathbb{E}_q\\left[\\log\\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\kappa^2 \\mathbf{I})\\right] \\tag{9}\n",
    "\\end{align*}$$\n",
    "\n",
    "Each term in the sum evaluates to\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}_q\\left[\\log  \\mathcal{N}(y_n|\\mathbf{w}^T\\mathbf{x}_n, \\sigma^2)\\right] &= \\mathbb{E}_q\\left[- \\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left(y_n - \\mathbf{w}^T \\mathbf{x}_n\\right)^2\\right]\n",
    "%\n",
    "=  - \\frac{1}{2} \\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\mathbb{E}_q\\left[y_n^2 + (\\mathbf{w}^T \\mathbf{x}_n)^2 - 2y_n \\mathbf{w}^T \\mathbf{x}_n\\right]\\tag{10}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "We can re-write the quadratic term as follows\n",
    "$$\\begin{align*}\n",
    "(\\mathbf{w}^T \\mathbf{x}_n)^2 = \\left(\\sum_{i=1}^D w_i x_{ni}\\right)^2 =  \\sum_{i=1}^D \\sum_{j=1}^D w_ix_{ni} w_j x_{nj} \\tag{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Substituting back yields\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}_q\\left[\\log  \\mathcal{N}(y_n|\\mathbf{w}^T\\mathbf{x}_n, \\sigma^2)\\right] &= - \\frac{1}{2} \\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} y_n^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^D \\sum_{j=1}^D \\mathbb{E}_q\\left[w_i w_j\\right] x_{ni}  x_{nj} +  \\frac{y_n}{\\sigma^2} \\mathbb{E}_q\\left[\\mathbf{w}^T\\right] \\mathbf{x}_n\\\\\n",
    "%\n",
    " &\\stackrel{(a)}{=}- \\frac{1}{2} \\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} y_n^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^D \\sum_{j=1}^D m_i m_j x_{ni}  x_{nj}  - \\frac{1}{2\\sigma^2} \\sum_{j=i}^D v_i x^2_{ni} +  \\frac{y_n}{\\sigma^2} \\mathbf{m}^T \\mathbf{x}_n\\\\\n",
    " %\n",
    " &=- \\frac{1}{2} \\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} y_n^2 - \\frac{1}{2\\sigma^2} (\\mathbf{m}^T \\mathbf{x}_n)^2  - \\frac{1}{2\\sigma^2} \\sum_{j=i}^D v_i x^2_{ni} +  \\frac{y_n}{\\sigma^2} \\mathbf{m}^T \\mathbf{x}_n\\\\\n",
    "%\n",
    "&= \\log \\mathcal{N}(y_n|\\mathbf{m}^T \\mathbf{x}_n, \\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{i=1}^D v_i x^2_{ni} \\tag{12}, \n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "where we have used the mean-field assumption in $(a)$, i.e. $\\mathbb{E}_q\\left[w_i w_j\\right] = \\mathbb{E}_{q(w_i)}\\left[w_i\\right]\\mathbb{E}_{q(w_j)}\\left[ w_j\\right] = m_i m_j$ for $i \\neq j$ and $\\mathbb{E}_q\\left[w_i^2\\right] = m_i^2 + v_i$ for $i = j$ such that:\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^D \\sum_{j=1}^D \\mathbb{E}_q\\left[w_i w_j\\right] x_{ni}  x_{nj} &= \\sum_{i=1}^D \\sum_{j \\neq i} \\mathbb{E}_q\\left[w_i w_j\\right] x_{ni}  x_{nj} + \\sum_{i=1}^D \\mathbb{E}_q\\left[w_i^2\\right] x_{ni}^2\\\\\n",
    "&= \\sum_{i=1}^D \\sum_{j \\neq i} m_i m_j x_{ni}  x_{nj} + \\sum_{i=1}^D \\left[m_i^2 + v_i\\right] x_{ni}^2 \\\\\n",
    "&= \\sum_{i=1}^D \\sum_{j=1}^D m_i m_j x_{ni}  x_{nj} + \\sum_{i=1}^D v_i x_{ni}^2\n",
    "\\end{align*}$$\n",
    "\n",
    " Don't hesitate to ask for help if some steps are unclear.\n",
    "\n",
    "\n",
    "A similar line of calculations yields the expectation for the log prior:\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_q\\left[\\ln\\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\kappa^2 \\mathbf{I})\\right] &= \\sum_{i=1}^D \\ln \\mathcal{N}(m_i|0, \\kappa^2) - \\frac{1}{2\\kappa^2} \\sum_{i=1}^D v_i \\tag{13}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Thus, we can now substitute the results from eq. (3), (12), and (13) to compute the analytical ELBO:\n",
    "\\begin{align}\n",
    "\\mathcal{L}\\left[q\\right]  = \\underbrace{\\sum_{n=1}^N \\ln \\mathcal{N}(y_n|\\mathbf{m}^T \\mathbf{x}_n, \\sigma^2)  -  \\frac{1}{2\\sigma^2}  \\sum_{i=1}^D \\sum_{n=1}^N v_i x^2_{ni}}_{\\text{expected log likelihood}} + \\underbrace{\\sum_{i=1}^D \\ln \\mathcal{N}(m_i|0, \\kappa^2) - \\frac{1}{2\\kappa^2} \\sum_{i=1}^D v_i}_{\\text{expected log prior}} + \\underbrace{\\frac12\\sum_{i=1}^D \\ln (2\\text{e}\\pi v_i)}_{\\text{entropy}} \\tag{14}\n",
    "\\end{align}\n",
    "\n",
    "We can then fit the variational approximation by optimizing the ELBO wrt. the means $m_i$ and variances $v_i$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**: Explain how the calculations of the expected log joint above would change if we had used a full rank Gaussian variational family, i.e. $q(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{\\Sigma})$, instead of the mean-field family. Note that you do **not** have to do the actual calculations, but just explain which terms would be influenced and how. [**Discussion question**]\n",
    "\n",
    "*Hints: What does expectations of the form $\\mathbb{E}_q\\left[w_iw_j\\right]$ evaluate to when $q$ is 1) a mean-field Gaussian distribution or 2) a full-rank Gaussian?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementating a variational approximation based on the analytical ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimize the lower bound in eq. (14) to fit the approximation and we will use the **autograd** library to compute the gradients of $\\mathcal{L}\\left[q\\right]$ wrt. the variational parameters automatically.\n",
    "\n",
    "The variational parameters are $\\mathbf{m}$ and $\\mathbf{v}$. Since the variances $v_i > 0$ must be non-negative, we parametrize the variance using $\\log v_i$ and collect all variational parameters in a vector $\\mathbf{\\lambda} = \\left\\lbrace \\mathbf{m}, \\log \\mathbf{v}\\right\\rbrace$ and optimize the ELBO wrt. $\\mathbf{\\lambda}  \\in \\mathbb{R}^{2D}$ using standard techniques for unconstrained optimization.\n",
    "\n",
    "In the first part of the exercise, we will consider the hyperparameters fixed and known: $\\kappa^2 = 10$ and $\\sigma^2=20$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**: Below you are given most of the code for computing the variational approximation of the linear Gaussian model. Complete the function `compute_ELBO` for evaluating the ELBO based on eq. (14)\n",
    "\n",
    "*Hints*\n",
    "- The function `log_npdf(x, m, v)` computes the log density of a univarate normal distribution with mean m and variance v at point x\n",
    "- The output of `compute_ELBO` must be a scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalInference(object):\n",
    "    \n",
    "    def __init__(self, num_params, step_size=1e-2, max_itt=2000, verbose=False, name='VariationalInference'):\n",
    "        \n",
    "        self.name = name\n",
    "        self.verbose = verbose        \n",
    "        self.X, self.y, self.ELBO = None, None, None\n",
    "    \n",
    "        # optimization settings\n",
    "        self.num_params, self.step_size, self.max_itt = num_params, step_size, max_itt\n",
    "        \n",
    "        # number of parameters to be optimized is 2*D\n",
    "        self.num_var_params = 2*self.num_params\n",
    "        \n",
    "        # Initialize the variational parameters for the mean-field approximation\n",
    "        self.m, self.v = np.zeros(num_params), np.ones(num_params)/self.num_params\n",
    "        self.lam = self.pack(self.m, self.v) # combine m and v into one vector\n",
    "        \n",
    "        # prepare optimizer and gradient\n",
    "        self.optimizer = AdamOptimizer(initial_param=self.lam, num_params=self.num_var_params, step_size=self.step_size)\n",
    "        self.compute_ELBO_gradient = grad(self.compute_ELBO)\n",
    "        \n",
    "    def pack(self, m, v):\n",
    "        \"\"\" pack all parameters into one big vector for (unconstrained) optimization as follows: lam = [m, log v] \"\"\"\n",
    "        lam = np.zeros(self.num_var_params)\n",
    "        lam[:self.num_params] = m\n",
    "        lam[self.num_params:2*self.num_params] = np.log(v)\n",
    "        return lam\n",
    "    \n",
    "    def unpack(self, lam):\n",
    "        \"\"\" unpack to mean and variance from lam = [m, log-v] \"\"\"\n",
    "        mean = lam[:self.num_params]\n",
    "        var = np.exp(lam[self.num_params:2*self.num_params])\n",
    "        return mean, var\n",
    "        \n",
    "    def compute_entropy(self, v=None):\n",
    "        \"\"\" compute entropy term \"\"\"\n",
    "        if v is None:\n",
    "            v = np.exp(self.lam[self.num_params:2*self.num_params])\n",
    "        return np.sum(0.5*np.log(2*np.pi*v) + 0.5)\n",
    "\n",
    "    def generate_posterior_samples(self, num_samples=1000):\n",
    "        return np.random.normal(self.m, np.sqrt(self.v), size=(num_samples, self.num_params))\n",
    "        \n",
    "    def compute_ELBO(self, lam):\n",
    "        \"\"\" computes the ELBO for the linear Gaussian model based on eq. (14).\n",
    "            \n",
    "            Input:\n",
    "            lam    -- np.array of variational parameters [m, log(v)]\n",
    "\n",
    "            Output:\n",
    "            ELBO   -- the ELBO value (scalar) for the specified variational parameters\n",
    "              \n",
    "             \"\"\"\n",
    "        \n",
    "        # unpack parameters from lambda-vector\n",
    "        m, v = self.unpack(lam)\n",
    "        \n",
    "        # implement model\n",
    "        sigma2, kappa2 = 20., 10.\n",
    "        expected_log_lik = <insert code here>\n",
    "        expected_log_prior = <insert code here>\n",
    "        entropy = self.compute_entropy(v)\n",
    "\n",
    "        return expected_log_lik + expected_log_prior + entropy\n",
    "\n",
    "    def fit(self, X, y, seed=0):\n",
    "        \"\"\" fits the variational approximation q given data (X,y) by maximizing the ELBO using gradient-based methods \"\"\" \n",
    "        np.random.seed(seed)\n",
    "        self.X, self.y, self.N = X, y, len(X)\n",
    "        self.ELBO_history, self.lam_history = [], []\n",
    "        \n",
    "        print('Fitting approximation using %s' % self.name)        \n",
    "        t0 = time()\n",
    "        for itt in range(self.max_itt):\n",
    "            \n",
    "            # evaluate ELBO\n",
    "            self.ELBO = self.compute_ELBO(self.lam)\n",
    "            \n",
    "            # store current values for plotting purposes\n",
    "            self.ELBO_history.append(self.ELBO)\n",
    "            self.lam_history.append(self.lam)\n",
    "\n",
    "            # compute gradient of ELBO wrt. variational parameters\n",
    "            g = self.compute_ELBO_gradient(self.lam)\n",
    "\n",
    "            # take gradient step\n",
    "            self.lam = self.optimizer.step(g)\n",
    "            \n",
    "            # verbose?\n",
    "            if self.verbose:\n",
    "                if (itt+1) % 250 == 0:\n",
    "                    print('\\tItt: %5d, ELBO = %3.2f' % (itt, np.mean(self.ELBO_history[-250:])))\n",
    "        \n",
    "        t1 = time()\n",
    "        print('\\tOptimization done in %3.2fs\\n' % (t1-t0))\n",
    "        \n",
    "        # track quantities through iterations for visualization purposes\n",
    "        self.ELBO_history = np.array(self.ELBO_history)\n",
    "        self.lam_history = np.array(self.lam_history)\n",
    "        self.m = self.lam[:self.num_params]\n",
    "        self.v = np.exp(self.lam[self.num_params:2*self.num_params])\n",
    "        self.m_history = self.lam_history[:, :self.num_params]\n",
    "        self.v_history = np.exp(self.lam_history[:, self.num_params:2*self.num_params])\n",
    "            \n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate a synthetic dataset and test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions and number of obserations\n",
    "D, N = 2, 500\n",
    "\n",
    "# create a synthetic data set and compute the exact posterior \n",
    "# w_true is the true weights used to generate the data: t = X*w_true + noise\n",
    "# m_true and S_true are the exact posterior mean and covariance, respectively\n",
    "X, y, w_true, m_true, S_true, exact_marginal = create_linear_regression_data(D, N)\n",
    "\n",
    "# run variational inference\n",
    "vi = VariationalInference(num_params=D, step_size=1e-2, max_itt=2000).fit(X, y)\n",
    "\n",
    "# compute relative error of the posterior mean\n",
    "def relative_error(w_est, w_true):\n",
    "    return np.sum((w_est-w_true)**2)/np.sum(w_true**2)\n",
    "\n",
    "print('Relative error for the posterior mean: %4.3e' % relative_error(vi.m, m_true))\n",
    "print('Relative error for the marginal posterior variances: %4.3e' % relative_error(vi.v, np.diag(S_true)))\n",
    "\n",
    "# check implementation\n",
    "assert(relative_error(vi.m, m_true) < 1e-6), \"The relative error of the posterior mean seems too large. Check your implementation of the ELBO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the implementation of the ELBO is correct, the relative error of the mean should be close to zero (smaller than $10^{-6}$). But note that the relative error for the variance is somewhat larger. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Black-box variational inference (BBVI) for the Linear Gaussian model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement BBVI. One of the major benefits of the BBVI is that we do not need to evaluate the ELBO and its gradient, we only have to implement a function for evaluating the log likelihood and log prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements isotropic Gaussian prior\n",
    "def log_prior_pdf(w, prior_var=10):\n",
    "    \"\"\" Evaluates the log prior Gaussian for each sample of w. \n",
    "        D denote the dimensionality of the model and S denotes the number of MC samples.\n",
    "\n",
    "        Inputs:\n",
    "            w             -- np.array of shape (S, D)\n",
    "\n",
    "        Returns:\n",
    "            log_prior     -- np.array of shape (S,)\n",
    "       \"\"\"\n",
    "    log_prior = np.sum(log_npdf(w, 0, prior_var), axis=1)\n",
    "    return log_prior\n",
    "\n",
    "# implemenents the log likelihood for linear regression\n",
    "def log_lik_linreg(X, y, w):\n",
    "    \"\"\"\n",
    "    Implements the log likelihood function for the linear regression model with Gaussian likelihood.\n",
    "    S is number of MC samples, N is number of datapoints in likelihood and D is the dimensionality of the model\n",
    "\n",
    "    Inputs:\n",
    "    X              -- Design matrix (np.array of size N x D)\n",
    "    y              -- vector of target (np.array of size N)\n",
    "    w              -- Matrix of weights (np.array of size S x D)\n",
    "\n",
    "    outputs:\n",
    "    log_likelihood -- Array of log likelihood for each sample in w (np.array of size S)\n",
    "     \"\"\"\n",
    "    # we consider the hyperparameters (sigma2) known and fixed to keep the code simple\n",
    "    sigma2 = 20\n",
    "    \n",
    "    # model components\n",
    "    log_likelihood = np.sum(log_npdf(y[:, None], X@w.T,  sigma2), axis=0)\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will re-use most of the code from *VariationalInference* class above and only re-implement the compute_ELBO function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackBoxVariationalInference(VariationalInference):\n",
    "    def __init__(self, log_prior, log_lik, num_params, step_size=1e-2, max_itt=2000, num_samples=20, batch_size=None, seed=0, verbose=False):\n",
    "    \n",
    "        # arguments specific to BBVI\n",
    "        self.log_prior = log_prior          # function for evaluating the log prior\n",
    "        self.log_lik = log_lik              # function for evaluating the log likelihood\n",
    "        self.num_samples = num_samples      # number of MC samples to use for estimation\n",
    "        self.batch_size = batch_size        # batch size\n",
    "        self.seed = seed                    # seed\n",
    "        \n",
    "        # pass remaining argument to VI class\n",
    "        super(BlackBoxVariationalInference, self).__init__(name=\"Black-box VI\", num_params=num_params, step_size=step_size, max_itt=max_itt, verbose=verbose)\n",
    "    \n",
    "    def compute_ELBO(self, lam):\n",
    "        \"\"\" compute an estimate of the ELBO for variational parameters in the *lam*-variable using Monte Carlo samples and the reparametrization trick.\n",
    "            \n",
    "            If self.batch_size is None, the function computes the ELBO using full dataset. If self.batch is not None, the function estimates the ELBO using a single minibatch of size self.batch_size. \n",
    "            The samples in the mini batch is sampled uniformly from the full dataset.\n",
    "            \n",
    "            inputs:\n",
    "            lam       --     vector of variational parameters (unconstrained space)\n",
    "\n",
    "            outputs:\n",
    "            ELBO      --     estimate of the ELBO (scalar)\n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "        # unpack variational parameters\n",
    "        m, v  = self.unpack(lam)\n",
    "        \n",
    "        # generate samples from epsilon ~ N(0, 1) and use re-parametrization trick\n",
    "        w_samples = <insert code here>\n",
    "\n",
    "        # prior term (scalar )\n",
    "        expected_log_prior_term = <insert code here>\n",
    "    \n",
    "        # batch mode or minibatching?\n",
    "        if self.batch_size:\n",
    "            # Use mini-batching\n",
    "            expected_log_lik_term = <insert code here>\n",
    "        else:\n",
    "            # No mini-batching\n",
    "            expected_log_lik_term = <insert code here>\n",
    "        \n",
    "        # compute ELBO\n",
    "        ELBO = expected_log_lik_term + expected_log_prior_term + self.compute_entropy(v)\n",
    "        \n",
    "        return ELBO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Complete the implementation of the function *compute_ELBO* above.\n",
    "\n",
    "*hints*: \n",
    "\n",
    "- *The *re-parametrization trick* states that we can replace $w_i \\sim \\mathcal{N}(w_i|m_i, v_i)$ with $w_i = m_i + v_i^{\\frac12} \\epsilon_i$ for $\\epsilon_i \\sim \\mathcal{N}(0, 1)$*.\n",
    "\n",
    "- *Start by implementing ELBO without mini-batching, test it, and once it works, come back and implement mini-batching*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the implementation of the class *VariationalInference* was hard-coded for the linear Gaussian model, whereas the class *BlackBoxVariationalInference* can easily be adapted to other models by providing the relevant log prior and log likelihood functions.\n",
    "\n",
    "Let's test it and compare with the algorithm from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "num_params = X.shape[1]\n",
    "max_itt = 200\n",
    "step_size = 5e-2\n",
    "num_samples = 7\n",
    "batch_size = None\n",
    "\n",
    "# fit approximations\n",
    "vi = VariationalInference(num_params, step_size, max_itt).fit(X, y)\n",
    "bbvi = BlackBoxVariationalInference(log_prior_pdf, log_lik_linreg, num_params, step_size, max_itt, num_samples, batch_size).fit(X, y)\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "axes[0].plot(bbvi.ELBO_history, color=colors[0], label='BBVI')\n",
    "axes[0].plot(vi.ELBO_history, linewidth=3, color=colors[1], label='VI')\n",
    "axes[0].axhline(exact_marginal, color='r', linestyle='--', label='Exact marginal likelihood', linewidth=3)\n",
    "axes[0].set(title='Evidence lower bound', xlabel='Iterations')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(bbvi.m_history[:, 0], bbvi.m_history[:, 1], '-', color=colors[0], label='BBVI', linewidth=3)\n",
    "axes[1].plot(vi.m_history[:, 0], vi.m_history[:, 1], '--', color=colors[1], label='VI', linewidth=3)\n",
    "axes[1].plot(vi.m_history[0, 0], vi.m_history[0, 1], 'ko', label='Initial value')\n",
    "\n",
    "axes[1].plot(m_true[0], m_true[1], 'r^', label='Exact posterior mean', markersize=15, alpha=0.75)\n",
    "axes[1].plot(w_true[0], w_true[1], 'g^', label='True weights', markersize=15, alpha=0.75)\n",
    "axes[1].set(xlabel='Posterior mean $m_1$', ylabel='Posterior mean $m_2$', title='Optimization trajectory for means')\n",
    "\n",
    "axes[2].plot(bbvi.v_history[:, 0], bbvi.v_history[:, 1], '-', color=colors[0], label='BBVI', linewidth=3)\n",
    "axes[2].plot(vi.v_history[:, 0], vi.v_history[:, 1], '--', color=colors[1], label='VI', linewidth=3)\n",
    "axes[2].plot(S_true[0,0], S_true[1,1], 'r^', label='Exact posterior variance', markersize=15, alpha=0.75)\n",
    "axes[2].plot(vi.v_history[0, 0], vi.v_history[0, 1], 'ko', label='Initial value')\n",
    "axes[2].set(xlabel='Posterior variance $v_1$', ylabel='Posterior variance $v_2$', title='Optimization trajectory for variances')\n",
    "\n",
    "for i in range(3):\n",
    "    axes.flat[i].legend()\n",
    "\n",
    "print(60*'-')\n",
    "print('VI')\n",
    "print(60*'-')\n",
    "print('Relative error for the posterior mean: %4.3e' % relative_error(vi.m, m_true))\n",
    "print('Relative error for the posterior variances: %4.3e' % relative_error(vi.v, np.diag(S_true)))\n",
    "print('\\n')\n",
    "\n",
    "print(60*'-')\n",
    "print('BBVI')\n",
    "print(60*'-')\n",
    "print('Relative error for the posterior mean: %4.3e' % relative_error(bbvi.m, m_true))\n",
    "print('Relative error for the posterior variances: %4.3e' % relative_error(bbvi.v, np.diag(S_true)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.2**: Explain the difference between the true weights, the exact posterior mean, and the approximate posterior mean. [**Discussion question**]\n",
    "\n",
    "**Task 4.3**: Experiment with different numbers of MC samples by changing the *num_samples* parameter, e.g. try *num_samples*$ = 1, 10, 100$ etc. for BBVI. What happens? [**Discussion question**]\n",
    "\n",
    "**Task 4.4**: Fix the number of samples to *num_samples*$ = 20$ and experiment with *batch_size*$ = \\text{None}, 1, 10, 100$. What happens? Increase the number of iterations if needed. [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Spam vs ham detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will build a simple spam vs ham (not spam) classifier based on a dataset of SMS messages. We'll see how BBVI can be extremely useful when prototyping different models.\n",
    "\n",
    "First, we will load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('./sms_data.npz')\n",
    "train_texts = data['train_texts']\n",
    "test_texts = data['test_texts']\n",
    "y_train = data['train_targets']\n",
    "y_test = data['test_targets']\n",
    "labels = data['labels']\n",
    "\n",
    "# find spam and ham examples\n",
    "spam_idx = np.where(y_train == 1)[0]\n",
    "ham_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "# print a few examples\n",
    "np.random.seed(123)\n",
    "\n",
    "print('Spam:')\n",
    "for n in np.random.choice(spam_idx, size=10, replace=False):\n",
    "    print(train_texts[n])\n",
    "print('')\n",
    "print('Ham:')\n",
    "for n in np.random.choice(ham_idx, size=10, replace=False):\n",
    "    print(train_texts[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will rely on *transfer learning* to predict the label of each text. First, we have used a pretrained FastText model to compute 300-dimensional embedding for each text. Afterwards, we have reduced the dimensionality to $D = 15$ (to speed up the computations). The FastText method is not part of the curriculum of this course, but if you are curiuos, you can read more about it here: [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html).\n",
    "\n",
    "As a result, we have a $D = 15$ vector representing each text. Let's inspect the first two components visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data['ztrain'] # embeddings for training set\n",
    "x_test = data['ztest']   # embeddings for test set\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.plot(x_train[y_train == 0, 0], x_train[y_train==0, 1], 'r.', label=labels[0])\n",
    "ax.plot(x_train[y_train == 1, 0], x_train[y_train==1, 1], 'b.', label=labels[1])\n",
    "ax.set(xlabel='PC1', ylabel='PC2', title='First two principal component of training set')\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the model\n",
    "\\begin{align*}\n",
    "    p(\\mathbf{y}, \\mathbf{w}) = \\prod_{n=1}^N p(y_n|\\mathbf{x}_n, \\mathbf{w})p(\\mathbf{w})=  \\prod_{n=1}^N p(y_n|\\mathbf{x}_n, \\mathbf{w})\\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1} \\mathbf{I})\\tag{15}\n",
    "\\end{align*}\n",
    "\n",
    "where $y_n \\in \\left\\lbrace 0, 1 \\right\\rbrace$ is the label of the $n$-th text, $\\mathbf{x}_n \\in \\mathbb{R}^{D}$ is the PCA features representation of each text, $p(y_n|\\mathbf{x}_n, \\mathbf{w})$ is the likeliood for the $n$-th observation and $p(\\mathbf{w})$ is a Gaussian prior on the weights. \n",
    "\n",
    "We will explore and compare three different likelihoods for binary classification:\n",
    "\n",
    "**Likelihood 1**: Standard Bernoulli distribution with a logistic sigmoid as inverse link function\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y_n|\\mathbf{w}, \\mathbf{x}_n) = \\text{Ber}\\left(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n)\\right) \\tag{16}\n",
    "\\end{align*}\n",
    "\n",
    "**Likelihood 2**: Standard Bernoulli distribution with a Probit function $\\mathbf{\\Phi}$ (CDF of standarized normal distribution) as inverse link function\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y_n|\\mathbf{w}, \\mathbf{x}_n) = \\text{Ber}\\left(y_n|\\Phi(\\mathbf{w}^T\\mathbf{x}_n)\\right) \\tag{16}\n",
    "\\end{align*}\n",
    "\n",
    "**Likelihood 3**: Robust likelihood with noisy labels\n",
    "\n",
    "The third likelihood we consider is a so-called *robust likelihood* that assumes a fraction of the training labels are mislabelled (e.g. samples that are actually spam are labelled as non-spam and vice versa), which can make the model more robust to outliers etc.\n",
    "\n",
    "Here we assume that a given training example, i.e. $y_i$, is mislabelled with probability $\\epsilon \\in \\left\\lbrace 0, 1\\right\\rbrace$. Let $e_i \\in \\left\\lbrace 0,1 \\right\\rbrace$ be $1$ if $y_i$ is mislabelled and otherwise $e_i = 0$. \n",
    "\n",
    "\\begin{align*}\n",
    "    p(y_i|\\mathbf{w}, \\mathbf{x}_i, e_i) &=    \\text{Ber}\\left(y_i|(1-e_i)\\sigma(\\mathbf{w}^T\\mathbf{x}_i) + e_i\\sigma(-\\mathbf{w}^T\\mathbf{x}_i)\\right)\\\\\n",
    "    p(e_i) &= \\text{Ber}(e_i|\\epsilon) \n",
    "\\end{align*}\n",
    "\n",
    "Finally, we can use the sum rule to marginalize out $e_i$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(y_i|\\mathbf{w}, \\mathbf{x}_i) &= \\sum_{e_i} p(y_i|\\mathbf{w}, \\mathbf{x}_i, e_i) p(e_i)\\\\\n",
    "%\n",
    "&= (1-\\epsilon)\\text{Ber}\\left(y_i|\\sigma(\\mathbf{w}^T\\mathbf{x}_i)\\right) + \\epsilon \\text{Ber}\\left(y_i|\\sigma(-\\mathbf{w}^T\\mathbf{x}_i)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Hence, this likelihood has an additional parameters $\\epsilon \\in \\left[0, 1\\right]$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now let $f = \\mathbf{w}^T \\mathbf{z}$ and plot likelihoods and log likelihoods for observing $y = 1$ for each the three likelihoods for $f \\in \\left[-10, 10\\right]$. Use $\\epsilon = 0.05$ for the robust model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihoods\n",
    "phi = lambda f: norm.cdf(f)\n",
    "sigmoid = lambda f: 1/(1+np.exp(-f))\n",
    "robust_sigmoid = lambda f, epsilon: (1-epsilon)*sigmoid(f) + epsilon*sigmoid(-f) \n",
    "\n",
    "# array for f\n",
    "f = np.linspace(-10, 10, 100)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "ax[0].plot(f, sigmoid(f), label='Sigmoid')\n",
    "ax[0].plot(f, phi(f), label='Probit')\n",
    "ax[0].plot(f, robust_sigmoid(f, 0.05), label='Robust')\n",
    "ax[0].legend()\n",
    "ax[0].set(ylabel='Likelihood for observing $y=1$', xlabel='f')\n",
    "\n",
    "ax[1].plot(f, np.log(sigmoid(f)), label='Sigmoid')\n",
    "ax[1].plot(f, np.log(phi(f)), label='Probit')\n",
    "ax[1].plot(f, np.log(robust_sigmoid(f, 0.05)), label='Robust')\n",
    "ax[1].legend()\n",
    "ax[1].set(ylabel='Log likelihood for observing $y=1$', xlabel='f');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.1**: Study the plots and comment on to which degree each of the three likelihoods penalize outliers. [**Discussion question**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6:  Prototyping and testing models using BBVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to implement all three models. We will start with the \"standard\" Bayesian logistic regression model with a Gaussian prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.scipy.special import logsumexp\n",
    "\n",
    "log_bernoulli_logit = lambda x, z: (1-x)*np.log(1-sigmoid(z)) + x*np.log(sigmoid(z))\n",
    "\n",
    "def log_lik1(X, y, w):\n",
    "    \"\"\"\n",
    "    Implements the log likelihood function for the logistic regression model with the sigmoid as inverse link function.\n",
    "    S is number of MC samples, N is number of datapoints in likelihood and D is the dimensionality of the model\n",
    "\n",
    "    Inputs:\n",
    "    X              -- Design matrix (np.array of size N x D)\n",
    "    y              -- vector of target (np.array of size N)\n",
    "    w              -- Matrix of weights (np.array of size S x D)\n",
    "\n",
    "    outputs:\n",
    "    log_likelihood -- Array of log likelihood for each sample in w (np.array of size S)\n",
    "     \"\"\"\n",
    "\n",
    "    # compute linear part (dim: S x N)\n",
    "    f = w @ X.T\n",
    "\n",
    "    # compute logits (dim: S x N)\n",
    "    logits = log_bernoulli_logit(y, f)\n",
    "\n",
    "    # sum across data points (dim: S)\n",
    "    log_likelihood = np.sum(logits, axis=1)   \n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "def predictive_model1(z, w_samples):\n",
    "    return np.mean(sigmoid(w_samples@z.T), axis=0)\n",
    "\n",
    "# settings\n",
    "num_params = 15\n",
    "max_itt = 5000\n",
    "step_size = 5e-2\n",
    "num_samples = 20\n",
    "batch_size = 200\n",
    "\n",
    "# set prior \n",
    "log_prior = lambda w: log_prior_pdf(w, 100.)\n",
    "\n",
    "# fit approximations\n",
    "bbvi1 = BlackBoxVariationalInference(log_prior, log_lik1, num_params, step_size, max_itt, num_samples, batch_size, verbose=True)\n",
    "bbvi1.fit(x_train, y_train);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's compute the accuracy and $F_1$-score ([$F_1$ scores on wiki](https://en.wikipedia.org/wiki/F-score)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prob, y):\n",
    "    # predict\n",
    "    yhat = 1.0 * (prob > 0.5)\n",
    "    # compute accuracy\n",
    "    return np.mean(yhat.ravel() == y.ravel())\n",
    "\n",
    "def compute_f1(prob, y):\n",
    "    # predict\n",
    "    yhat = 1.0 * (prob > 0.5)\n",
    "    # evaluate precision/recall for f1\n",
    "    tp = np.logical_and(yhat == 1, y == 1).sum()\n",
    "    fp = np.logical_and(yhat == 1, y == 0).sum()\n",
    "    fn = np.logical_and(yhat == 0, y == 1).sum()\n",
    "    precision, recall = tp/(tp+fp), tp/(tp+fn)\n",
    "    # compute f1\n",
    "    return 2* (precision*recall)/(precision+recall)\n",
    "\n",
    "w1_samples = bbvi1.generate_posterior_samples()\n",
    "p1_test = predictive_model1(x_test, w1_samples)\n",
    "\n",
    "print(f'M1: Test  accuracy = {compute_accuracy(p1_test, y_test):3.2f}, Test  F1 = {compute_f1(p1_test, y_test):3.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.1**: Implement the probit regression model. Fit the model using BBVI and compute the test metrics\n",
    "\n",
    "*Hints*:\n",
    "- remember to both implement the log joint and the function for the evaluating predictive distribution\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2**: Implement the robust classification model for $\\epsilon = 0.05$ and evaluate the test metrics\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we assumed $\\epsilon = 0.05$. In practice, one could learn this way by either simply optimizing the ELBO wrt. $\\epsilon$ (corresponding to a marginal likelihood estimate). A more Bayesian approach would be to impose a Beta-prior distribution on $\\epsilon$ and compute the posterior distribution using BBVI as well. Furthermore, it is possible to get much better performance in this dataset, but the purpose of the exercise is to learn BBVI and see how easy it is to prototype different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.4**: Suppose we decided to augment the model with a prior distribution for $\\epsilon \\in \\left[0, 1\\right]$, e.g. $p(\\epsilon) = \\text{Beta}(\\epsilon|a,b)$. How would you adapt the variational family?\n",
    "\n",
    "**Hints**: What is the support a mean-field Gaussian distribution?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
