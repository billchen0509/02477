{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as snb\n",
    "import time\n",
    "\n",
    "from autograd.misc.optimizers import adam\n",
    "from autograd.scipy.special import logsumexp\n",
    "from autograd import grad\n",
    "from autograd import hessian\n",
    "from autograd.misc import flatten\n",
    "\n",
    "from nn import NeuralNetwork\n",
    "\n",
    "# style stuff\n",
    "snb.set_style('darkgrid')\n",
    "snb.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this exercise we will study Bayesian-inspired methods for deep learning. The exercise is divided into the following parts:\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Introduction and theory\n",
    "- Part 2: Fitting a neural network to a toy problem with MAP-inference\n",
    "- Part 3: Last layer Laplace approximations (LLLA)\n",
    "- Part 4: Deep ensembles\n",
    "- Part 5: Modelling the Concrete dataset from the UCI repository\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n",
    "**Note**: If you find yourself spending more than 30minutes on a single task, consider asking for help or look up at the solution for inspiration and move forward.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Introduction and theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will explore Bayesian-inspired methods for deep learning. The core concept in Bayesian methods is to compute weighted averages wrt. the posterior distribution of the parameters instead of \"betting everything\" on a single set of parameters, **a point estimate**. We will focus on supervised learning with probabilistic models of the form\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = \\prod_{n=1}^N p(y_n|\\mathbf{w})p(\\mathbf{w}) \\tag{1},\n",
    "\\end{align*}\n",
    "\n",
    "where $p(y_n|\\mathbf{w})$ is the likelihood for the $n$-th observation, which is parametrized by a neural network with parameters $\\mathbf{w}$. Ideally, we would like to compute the posterior distribution $p(\\mathbf{w}|\\mathbf{y})$ and use this to compute the posterior predictive distribution, but this is generally a very difficult task for neural network models due to the high-dimensional nature of the models, the complexity posterior geometry, and scale of modern datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Non-linear regression models with heteroscedastic noise**\n",
    "\n",
    "In this exercise, we will study non-linear regression models with Gaussian likelihoods of the form\n",
    "\n",
    "\\begin{align*}\n",
    "    p(y_n|\\mathbf{w}, \\mathbf{x}_n) = \\mathcal{N}(y_n|\\mu_{\\mathbf{w}}(\\mathbf{x}_n), \\sigma^2_{\\mathbf{w}}(\\mathbf{x}_n)). \\tag{2}\n",
    "\\end{align*}\n",
    "\n",
    "In this model, we assume **heteroscedasticity**, i.e. we allow the noise variance of the observations to depend on the input $\\mathbf{x}$. This should be contrasted with **homoscedastic models**, where the noise variance is assumed to be constant, i.e. $\\sigma^2(\\mathbf{x}_n) = \\sigma^2_0$. Note that this is the **variance of the likelihood**, i.e. the aleatoric component of the uncertainty, so this should not be confused with epistemic uncertainty, which can also depend on $x$.\n",
    "\n",
    "We will parameterize this likelihood using a neural network with two outputs, where the first output represents the mean function of the Gaussian distribution and the second represent the log variance of the Gaussian distribution. That is, if $f(\\mathbf{x}|\\mathbf{w}): \\mathbb{R}^D \\rightarrow \\mathbb{R}^2$ is a neural network with parameters $\\mathbf{w}$ and two output nodes, then\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{\\mathbf{w}}(\\mathbf{x}) &= f_{1}(\\mathbf{x}|\\mathbf{w}) \\tag{3}\\\\\n",
    "\\log \\sigma^2_{\\mathbf{w}}(\\mathbf{x}) &= f_{2}(\\mathbf{x}|\\mathbf{w}).\n",
    "\\end{align*}\n",
    "\n",
    "This construction implies that $\\sigma^2_{\\textbf{w}}(\\mathbf{x}) = e^{f_{2}(\\mathbf{x}|\\mathbf{w})} > 0$, which ensures that the predicted noise variance will always be strictly positive no matter what the neural network outputs.\n",
    "\n",
    "\n",
    "We will use a fully-connected neural network with two hidden layers:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_1 &= h(\\mathbf{W}_0\\mathbf{x} + \\mathbf{b}_0),\\\\\n",
    "\\mathbf{z}_2 &= h(\\mathbf{W}_1\\mathbf{z}_1 + \\mathbf{b}_1),\\\\\n",
    "\\mathbf{f} &= \\mathbf{W}_2\\mathbf{z}_2 + \\mathbf{b}_2, \\tag{4}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $\\mathbf{f}\\in \\mathbb{R}^2$ and $\\mathbf{W}_0$ and $\\mathbf{b}_0$ are the weights and biases for the first layer etc and $h$ is a non-linear activation function, which we will take to be the tangent hyperbolic function. We assume an isotropic Gaussian prior on all parameters, i.e. if $\\mathbf{w} = \\left\\lbrace \\mathbf{W}_0, \\mathbf{b}_0, \\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2  \\right\\rbrace$ denotes all the parameters of the network, then\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) \\tag{5}\n",
    "\\end{align*}\n",
    "for $\\alpha > 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximate inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Bayesian deep learning, we aim to find a tractable approximation $q(\\mathbf{w})$ such that we can approximate the posterior predictive distribution for a new point $y^*$ as follows\n",
    "\n",
    "\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int p(y^*|\\mathbf{w}, \\mathbf{x}^*) q(\\mathbf{w}) \\text{d}\\mathbf{w} \\tag{6}.\n",
    "\\end{align*}\n",
    "\n",
    "For example, for Laplace approximations and Gaussian variational approximations, we have $q(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{V})$ and the integral in eq. (6) can be approximated using Monte Carlo sampling \n",
    "\n",
    "\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) \\approx \\frac{1}{S} \\sum_{i=1}^S p(y^*|\\mathbf{w}^{(i)}, \\mathbf{x}^*), \\quad\\quad\\text{where}\\quad\\quad \\mathbf{w}^{(i)} \\sim  q(\\mathbf{w}) \\tag{7}.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Deep Ensembles**\n",
    "\n",
    "An alternative to Gaussian posterior approximations is **Deep Ensembles** (DE), where we simply fit our model $S$-times from initial parameters (e.g. by using different seeds) and treat the resulting parameters vectors $\\mathbf{w}^{(i)}$ for $i = 1, \\dots, S$ as pseudo-Monte Carlo samples corresponding to a posterior approximation of the form $q_{\\text{DE}}(\\mathbf{w}) = \\frac{1}{S}\\sum_{i=1}^S \\delta(\\mathbf{w}-\\mathbf{w}^{(i)})$,\n",
    "where $\\delta(\\cdot)$ is Dirac's delta function. Thus, for deep ensembles, we approximate the posterior predictive distribution as follows\n",
    "\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int p(y^*|\\mathbf{w}, \\mathbf{x}^*) q_{\\text{DE}}(\\mathbf{w}) \\text{d}\\mathbf{w} = \\frac{1}{S}\\sum_{i=1}^S  p(y^*|\\mathbf{w}^{(i)}, \\mathbf{x}^*). \\tag{8}\n",
    "\\end{align*}\n",
    "\n",
    "MAP-inference is equivalent to a \"degenerate\" deep ensemble with $S = 1$ and therefore, we can think of MAP-inference as approximate posterior inference with the degenerate approximation $q_{\\text{MAP}}(\\mathbf{w}) = \\delta(\\mathbf{w}- \\mathbf{w}_{\\text{MAP}})$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) \\approx \\int p(y^*|\\mathbf{w}, \\mathbf{x}^*) q_{\\text{MAP}}(\\mathbf{w}) \\text{d}\\mathbf{w} = p(y^*|\\mathbf{w}_{\\text{MAP}}, \\mathbf{x}^*), \\tag{8b}\n",
    "\\end{align*}\n",
    "\n",
    "which we also know as a **plugin approximation**.\n",
    "\n",
    "**Task 1.1**: Suppose you have trained a deep ensemble with $S = 10$ models and let $\\mathbf{f}^{(i)} \\in \\mathbb{R}^2$ denote the output of the neural network for the $i$'th model for a given input $\\mathbf{x}^*$. Determine the expression for the resulting approximate posterior predictive distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Last-layer Laplace approximations***\n",
    "\n",
    "In this exercise, we will compare standard **MAP-inference** with **deep ensembles** and **Last Layer Laplace Approximations** (LLLA). To construct a Laplace approximation, we need to evaluate to Hessian of the log joint distribution, which can be prohibitively expensive for even moderately sized networks. The LLLA approximation is a cheap alternative, where we treat all layers as deterministic except for the very last layer. That is, for our network with 2 hidden layers, we treat the parameters $\\mathbf{W}_0, \\mathbf{b}_0, \\mathbf{W}_1,$ and $\\mathbf{b}_1$ as deterministic and only apply the Laplace approximation for the parameters of the last layer, i.e. $\\mathbf{W}_2$ and $\\mathbf{b}_2$. Consequently, we only need to estimate a much smaller Hessian matrix. Furthermore, LLLA approximation has the additional benefits that it can easily be added *post-hoc* to a pretrained model.\n",
    "\n",
    "Let $L$ be the number of layers and let $\\mathbf{z}_2 \\in \\mathbb{R}^H$, then we can separate the network into two functions: A function $f_{1:L-1}$ that maps from the input to output of the second hidden layer, i.e.  $\\mathbf{z}_2 = f_{1:L-1}(\\mathbf{x}): \\mathbb{R}^D \\rightarrow \\mathbf{R}^H$, and a function $f_{L}$ that maps $\\mathbf{z}_2$ to the output of the network, i.e. $f_L: \\mathbb{R}^H \\rightarrow \\mathbb{R}^O$ defined by\n",
    "\n",
    "\\begin{align*}\n",
    "f_L(\\mathbf{z}_2|\\mathbf{w}_L) &= \\mathbf{W}_2\\mathbf{z}_2 + \\mathbf{b}_2,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{w}_L = \\left\\lbrace \\mathbf{W}_2, \\mathbf{b}_2 \\right\\rbrace$ is the parameters of the layer layer and $\\mathbf{w}_{1:L-1}$ is the parameters of the rest of the network. That is, to apply LLLA, we first compute the MAP estimate of the full network, i.e. $\\mathbf{w}_{\\text{MAP}}$, but we only have to compute the Hessian wrt. $\\mathbf{w}_L$.\n",
    "\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "A common way to evaluate probabilistic models is using the average *log predictive density* (LPD) for a dataset (arguments for this metric can be found in [Evaluating Predictive Uncertainty Challenge](https://www.quinonero.net/Publications/quinonero06epuc.pdf), Quiñonero-Candela et al., 2006). For a set of posteriors samples $\\left\\lbrace \\mathbf{w}^{(i)} \\right\\rbrace_{i=1}^S$ (no matter whether they come from a Laplace approximation or deep ensembles etc), we can compute the posterior distribution of $y$ by computing the output of the network for each of the weight samples $\\mathbf{w}^{(i)}$ to get\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\lbrace f^{(i)}(\\mathbf{x}^*) \\right\\rbrace_{i=1}^S \\tag{9}\n",
    "\\end{align*}\n",
    "\n",
    " for a test point $\\mathbf{x}^*$. Finally, we can use these samples to measure the log predictive density as follows\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LPD} = \\log p(y^*|\\mathbf{y}, \\mathbf{x}^*) \\approx \\log \\left[ \\frac{1}{S}\\sum_{i=1}^S  p(y^*|\\mathbf{w}^{(i)}, \\mathbf{x}^*) \\right].\n",
    "\\end{align*} \n",
    "\n",
    "For our specific model, which outputs the mean and log variance of the Gaussian predictive distribution, the log predictive density for $y^*$ becomes\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LPD} = \\log \\left[\\frac{1}{S}\\sum_{i=1}^S \\mathcal{N}(y^*|f_1^{(i)}(\\mathbf{x}^*), e^{f_2^{(i)}(\\mathbf{x}^*)})\\right].\\tag{10}\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Fitting a neural network to a toy problem with MAP-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a small toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('./ex13_toydata.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "Xp = data['Xp']\n",
    "y_train = data['t_train']\n",
    "y_test = data['t_test']\n",
    "\n",
    "\n",
    "# the data is generating using a sigmoid as underlying signal\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "f = lambda x: sigmoid(x)\n",
    "\n",
    "\n",
    "def plot_data(ax):\n",
    "    ax.plot(Xp, f(Xp), color=plt.cm.tab20(6), ls=\"--\", linewidth=2, label='True function')\n",
    "    ax.scatter(X_test, y_test, s=15, color=plt.cm.tab20(0), label='Test data')\n",
    "    ax.scatter(X_train, y_train, s=15, color=plt.cm.tab20(2), label='Training data')\n",
    "    ax.set(xlabel='Input feature x', ylabel='Target variable y', ylim=(-0.3, 2))\n",
    "    \n",
    "# plot data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_data(ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**: Consider the dataset above. Why would the assumption of **homoscedastic noise** be inappropriate here? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: Complete the implementation of the log likelihood function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_npdf(x, m, v):\n",
    "    return -0.5*(x-m)**2/v - 0.5*np.log(2*np.pi*v)\n",
    "\n",
    "def log_lik_regression(f, y):\n",
    "    \"\"\" implement the log likelihood in eq. (2)  for network outputs f and observations y\n",
    "    \n",
    "    inputs:\n",
    "    f           --  output of neural network (np.array: N x 2)\n",
    "    y           --  targets/observations     (np.array: N x 1)\n",
    "\n",
    "    output:\n",
    "    log_lik     --  sum of log likelihoods terms for observations in y (np.float64: scalar)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "     log_lik = <insert code here>\n",
    "\n",
    "    return log_lik\n",
    "\n",
    "\n",
    "# simple sanity checks for your implementation\n",
    "assert type(log_lik_regression(np.ones((len(y_train), 2)), y_train)) is np.float64, f\"The output of log_lik_regression must be a scalar (float), but the output was {type(log_lik_regression(np.ones((len(y_train), 2)), y_train))}. Please verify that your implementation is correct.\"\n",
    "assert np.allclose(log_lik_regression(np.ones((len(y_train), 2)), y_train), -150.62110358638134), f\"The output of log_lik_regression must be  close to -150.62110358638134, but the output was {log_lik_regression(np.ones((len(y_train), 2)), y_train)}. Please verify that your implementation is correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will compute the MAP-solution of the neural network by optimizing the log joint distibution wrt. $\\mathbf{w}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify architecture\n",
    "num_inputs = 1\n",
    "hidden1 = 20\n",
    "hidden2 = 20\n",
    "num_outputs = 2\n",
    "network_shape = [num_inputs, hidden1, hidden2, num_outputs]\n",
    "\n",
    "# create and train network\n",
    "model_MAP = NeuralNetwork(X_train, y_train, network_shape, log_lik_regression, alpha=1., step_size=1e-2, batch_size=None, num_iters=2000, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function `m.predict` to compute a forward pass through the network. That is, it evaluates $f(\\mathbf{x})$ for the input points in the array `Xp` using $\\mathbf{w}_{\\text{MAP}}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_MAP.predict(Xp)\n",
    "print(f'shape of Xp: {Xp.shape}')\n",
    "print(f'shape of pred: {pred.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output variable `pred` is an $N \\times 2$ array because the network has two outputs. \n",
    "\n",
    "For the MAP-method, we can easily compute the predictive mean and resulting intervals:\n",
    "\n",
    "- For each input point $\\mathbf{x}$, we predict a mean value $\\mu(\\mathbf{x})$ and a variance $\\sigma^2(\\mathbf{x})$ of a Gaussian.\n",
    "\n",
    "- A $95%$-interval for a Gaussian $\\mathcal{N}(x|m, v)$ is approximately $\\left[m - 1.96\\sqrt{v}, m + 1.96\\sqrt{v}\\right]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_and_var(pred, samples=False):\n",
    "    if samples:\n",
    "        # if samples == True, pred is expected to be of shape [num_samples, P, 2], where P is the number of input vectors\n",
    "        return pred[:, :, 0], np.exp(pred[:, :, 1])\n",
    "    else:\n",
    "        # if samples == False, pred is expected to be of shape [P, 2], where P is the number of input vectors\n",
    "\n",
    "        return pred[:, 0], np.exp(pred[:, 1])\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_data(ax)\n",
    "\n",
    "# separate mean and variance \n",
    "mu_MAP, v_MAP = extract_mean_and_var(pred)\n",
    "\n",
    "# plot\n",
    "lower_MAP, upper_MAP = mu_MAP - 1.96*np.sqrt(v_MAP), mu_MAP + 1.96*np.sqrt(v_MAP)\n",
    "ax.plot(Xp, mu_MAP)\n",
    "ax.fill_between(Xp.ravel(), lower_MAP, upper_MAP, color='b', alpha=0.2)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: How can you tell from the resulting fit that we have used a heteroscedastic model? [**Discussion question**]\n",
    "\n",
    "Next, let's compute the average LPD for both the training and test set.\n",
    "\n",
    "**Task 2.4**: Compute the average LPD for the training and the test sets.\n",
    "\n",
    "*Hints*:\n",
    "- Recall that $q_{\\text{MAP}}(\\mathbf{w}) = \\delta(\\mathbf{w}- \\mathbf{w}_{\\text{MAP}})$ and use eq. (8b) and (10).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Last layer Laplace approximations (LLLA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will study the last-layer Laplace approximation. The model class `NeuralNetwork` has two convenient functions for this purpose: `first_layers(params, X)` and `last_layer(param_last_layer, z)`, which implements $f_{1:L-1}$ and $f_L$, respectively. \n",
    "\n",
    "The cell below provides a simple implementation of LLLA. A couple of details of about the implementation:\n",
    "\n",
    " - The `model.params` variable stores all parameters of the network as a list: [[$\\mathbf{W}_0, \\mathbf{b}_0$], [$\\mathbf{W}_1, \\mathbf{b}_1$], [$\\mathbf{W}_2, \\mathbf{b}_2$]].\n",
    " - The function `flatten` takes a list of parameters as input and returns 1) a flattened version of the list (i.e. a vector), and 2) an `unflatten` function to the reverse this operation.\n",
    " - We need the parameters in flattened form to be able to compute the Hessian using autograd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LLLA(model):\n",
    "    \"\"\" compute last-layer Laplace approximation based on a pretrained model object with L layers \"\"\"\n",
    "\n",
    "    # get full list of parameters for network and extract parameters of last layer\n",
    "    full_params = model.params\n",
    "    last_params = full_params[-1]\n",
    "\n",
    "    # flatten parameters of the last layer to prepare for the approximation\n",
    "    w_last_flat, unflatten = flatten(last_params)\n",
    "\n",
    "    # compute activation after first L-1 layers using input features and parameters\n",
    "    z = model.first_layers(full_params, model.X)\n",
    "\n",
    "    # prepare for Hessian computation by constructing function for evaluating the last layer of the neural network\n",
    "    def last_layer_obj(w_):\n",
    "        p = unflatten(w_)                       # unflatten\n",
    "        y = model.last_layer(p, z)              # map through last layer\n",
    "        return -model.log_lik_fun(y, model.t)\n",
    "\n",
    "    # compute Hessian of log likelihood\n",
    "    H_loglik = hessian(last_layer_obj)(w_last_flat)\n",
    "\n",
    "    # comptue Hessian of log prior\n",
    "    H_logprior = model.alpha * np.identity(len(w_last_flat))\n",
    "\n",
    "    # compute Hessian of log joint\n",
    "    H = H_loglik + H_logprior\n",
    "    \n",
    "    # can be done more efficiently depending on the Hessian approximation, but will be less transparent.\n",
    "    S = np.linalg.inv(H)\n",
    "\n",
    "    # return params of last layer (flattened), estimated covariance matrix, the estimated hessian and a function for unflatting the parameters\n",
    "    return w_last_flat, S, H, unflatten\n",
    "\n",
    "\n",
    "w_MAP_flat, S_LLLA, H, unflatten = compute_LLLA(model_MAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the array `w_MAP_flat` and  the matrix `S_LLLA` contain the mean and covariance matrix of the posterior approximation of the last-layer, respectively.\n",
    "\n",
    "**Task 3.1**: Fill in the missing pieces in the implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LLLA(X, model, w_LLLA_flat, S_LLLA, num_samples=1000):\n",
    "    \"\"\" Computes the samples from the LLLA predictive distribution using a pretrained model object \n",
    "        and an LLLA approximation specificied by a mean vector w_LLLA_flat and a covariance matrix S_LLLA.\n",
    "        B denotes the number of parameters in the last layer\n",
    "\n",
    "    inputs:\n",
    "    X            ---    input points (np.array: P x D)\n",
    "    model        ---    NeuralNetwork object (pretrained) \n",
    "    w_LLLA_flat  ---    vector of mean parameters for the last layer (np.array: B x 1)\n",
    "    S_LLLA_flat  ---    covarince matrix for the parameters of the last laeyr (np.array: (B x B))\n",
    "    num_samples  ---    number of samples to generate for each input\n",
    "\n",
    "    outputs:\n",
    "    preds_LLLA   ---    samples from the predictive distribution for each input in X (np.array: num_samples x P x 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # first feed the inputs through the first part of network\n",
    "    z = <insert code here>\n",
    "\n",
    "    # then we will sample from the approximate posterior of the last layer\n",
    "    w_samples = <insert code here>\n",
    "\n",
    "    # compute predictions for each posterior sample\n",
    "    preds_LLLA = np.stack([model.last_layer(unflatten(w_i), z) for w_i in w_samples]) \n",
    "\n",
    "    \n",
    "    return preds_LLLA\n",
    "\n",
    "\n",
    "# make predictions for plotting\n",
    "preds_LLLA_pred = predict_LLLA(Xp, model_MAP, w_MAP_flat, S_LLLA)\n",
    "\n",
    "# sanity check\n",
    "assert(preds_LLLA_pred.shape == (1000, 100, 2)), f\"The shape of preds_LLLA_pred must be (1000, 100, 2), but was found to be {preds_LLLA_pred.shape}. Go back and check your implementation.\"\n",
    "\n",
    "# split into mean and variance\n",
    "mu_LLLA_pred, v_LLLA_pred = extract_mean_and_var(preds_LLLA_pred, samples=True)\n",
    "print(f'shape of mu_LLLA_pred: {mu_LLLA_pred.shape}')\n",
    "print(f'shape of  v_LLLA_pred: {v_LLLA_pred.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two arrays above now contains $S= 1000$ samples of the mean and variance of the Gaussian distribution for each of the 100 points in the array `Xp`. We can use those for plotting the predictive distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(ax, mean, variances, color='b', seed=0):\n",
    "    \"\"\" Plot the predictive distribution (mean and 95% credibility interval) obtained.\n",
    "        Below, S denotes the number of samples and P denoted the number of points. For each of the S samples and\n",
    "        for each of the P points, the function generates a sample from corresponding Normal distribution and computes the mean and 95% interval and plots it on ax.\n",
    "        \n",
    "    inputs:\n",
    "    ax          -- axis object for plotting\n",
    "    mean        -- array of mean values (np.array: S x P)\n",
    "    variances   -- array of variance values (np.array: S x P)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    f = np.random.normal(mean, np.sqrt(variances))\n",
    "    f_mean  = np.mean(f, axis=0)\n",
    "    f_lower = np.percentile(f, 2.5, axis=0)\n",
    "    f_upper = np.percentile(f, 97.5, axis=0)\n",
    "\n",
    "    ax.plot(Xp, f_mean, color=color)\n",
    "    ax.fill_between(Xp.ravel(), f_lower, f_upper, alpha=0.2, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the predictions for MAP and LLLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(25, 6))\n",
    "\n",
    "# first panel: posterior predictive via plugin approximation with MAP solution\n",
    "plot_data(ax[0])\n",
    "ax[0].plot(Xp, mu_MAP)\n",
    "ax[0].fill_between(Xp.ravel(), lower_MAP, upper_MAP, color='b', alpha=0.2)\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Predictive uncertainty for MAP')\n",
    "\n",
    "# second panel: posterior samples of f1(x), i.e. mean of y(x)\n",
    "plot_data(ax[1])\n",
    "ax[1].plot(Xp, mu_LLLA_pred[:200, :].T, 'b', alpha=0.1);\n",
    "ax[1].set_title('Posterior samples from $f_1(\\mathbf{x})$')\n",
    "\n",
    "# third panel: posterior samples of exp(0.5 f2(x)), i.e. std. dev of y(x)\n",
    "ax[2].plot(Xp, np.sqrt(v_LLLA_pred)[:200, :].T, 'b', alpha=0.1);\n",
    "ax[2].set(xlabel='Input feature x', ylabel='Noise std. dev.', title='Posterior samples from $e^{\\\\frac{1}{2} f_2(\\mathbf{x})}$')\n",
    "\n",
    "# right-most: posterior predictive via LLLA\n",
    "plot_data(ax[3])\n",
    "plot_predictions(ax[3], mu_LLLA_pred, v_LLLA_pred)\n",
    "ax[3].set_title('Predictive uncertainty for LLLA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**: Comment on the resulting fits. How does the MAP and LLLA compare? How well do the models fit in region with training data and in regions without training data? Does the uncertainty look reasonable? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3**:  Which of the four plots visualizes (some degree of) aleatoric uncertainty, epistemic uncertainty or a combination? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the training and test LPD for LLLA.\n",
    "\n",
    "**Task 3.4**: Compute the average LPD for the training and test set for the LLLA approximation. How does the LLLA approximation compare with the MAP solution in terms of LPD?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Deep ensembles\n",
    "\n",
    "Finally, we will implement a deep ensemble with 10 models and compare to MAP and LLLA solutions.\n",
    "\n",
    "Below you are given a simple template for implementing a deep ensemble:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEnsemble(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.num_models = 0\n",
    "\n",
    "    def add_model(self, model):\n",
    "        \"\"\" # add pretrained model\"\"\"\n",
    "        self.models.append(model)\n",
    "        self.num_models = len(self.models)\n",
    "        \n",
    "    def predict_single(self, Xp, idx_model):\n",
    "        \"\"\" predict using a single model in the ensemble \"\"\"\n",
    "        return self.models[idx_model].predict(Xp)\n",
    "\n",
    "    def predict(self, Xp, subset_models=None):\n",
    "        # use all models in the ensemble or only a subset?\n",
    "        if subset_models is None:\n",
    "            list_of_models = range(self.num_models)\n",
    "        else:\n",
    "            list_of_models = range(subset_models)\n",
    "\n",
    "        # compute and return predictions for each model\n",
    "        return [self.predict_single(Xp, idx_model) for idx_model in list_of_models ]\n",
    "\n",
    "    def sample_predictive_dist(self, Xp, subset_models=None, num_samples=1000):\n",
    "        \"\"\" predict using all models in the ensemble (or a subset of them) \"\"\"\n",
    "\n",
    "        # make predictions for models\n",
    "        preds = self.predict(Xp, subset_models)\n",
    "\n",
    "        # sample predictive distribution for each model\n",
    "        y_samples = []\n",
    "        for idx_model in range(len(preds)):\n",
    "            mu, var = extract_mean_and_var(preds[idx_model])\n",
    "            y = np.random.normal(mu, np.sqrt(var), size=(num_samples, len(mu)))\n",
    "            y_samples.append(y)\n",
    "    \n",
    "        # ... and concate the results\n",
    "        return np.concatenate(y_samples, axis=0)\n",
    "    \n",
    "# prepare deep ensemble\n",
    "num_models = 10\n",
    "ensemble_model = DeepEnsemble()\n",
    "for idx_model in range(num_models):\n",
    "    print(f'Training model {idx_model+1}/{num_models}')\n",
    "    ensemble_model.add_model(NeuralNetwork(X_train, y_train, network_shape, log_lik_regression, alpha=1., step_size=1e-2, batch_size=None, num_iters=2000, seed=idx_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions for all models in the ensemble\n",
    "preds = ensemble_model.predict(Xp)\n",
    "\n",
    "# extract mean predictions from individual models for plotting purposes\n",
    "mu_ensemble, var_ensemble = extract_mean_and_var(np.stack(preds), True)\n",
    "\n",
    "# generate samples from the predictive distribution\n",
    "y_samples_ensemble = ensemble_model.sample_predictive_dist(Xp)\n",
    "\n",
    "# compute mean and 95% interval\n",
    "y_mean_ensemble = np.mean(y_samples_ensemble, axis=0)\n",
    "lower_ensemble, upper_ensemble = np.percentile(y_samples_ensemble, [2.5, 97.5], axis=0)\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25, 6))\n",
    "\n",
    "# first panel: posterior predictive via plugin approximation with MAP solution\n",
    "plot_data(ax[0])\n",
    "ax[0].set_title('Predictive uncertainty for MAP')\n",
    "ax[0].plot(Xp, mu_MAP)\n",
    "ax[0].fill_between(Xp.ravel(), lower_MAP, upper_MAP, color='b', alpha=0.2)\n",
    "ax[0].legend()\n",
    "\n",
    "# second panel: posterior samples of f1(x), i.e. mean of y(x)\n",
    "plot_data(ax[1])\n",
    "ax[1].set_title('Posterior samples of $f_1(\\mathbf{x}))$ for DE')\n",
    "ax[1].plot(Xp, mu_ensemble.T, 'b', alpha=0.5);\n",
    "\n",
    "# third panel: posterior samples of exp(0.5 f2(x)), i.e. std. dev of y(x)\n",
    "ax[2].set(xlabel='Input feature x', ylabel='Noise std. dev.', title='Posterior samples from $e^{\\\\frac{1}{2} f_2(\\mathbf{x}))}$ for DE')\n",
    "ax[2].plot(Xp, np.sqrt(var_ensemble).T, 'b', alpha=0.5);\n",
    "\n",
    "# fourth panel: posterior predictive via DE\n",
    "plot_data(ax[3])\n",
    "ax[3].set_title('Predictive uncertainty for DE')\n",
    "ax[3].plot(Xp, y_mean_ensemble)\n",
    "ax[3].fill_between(Xp.ravel(), lower_ensemble, upper_ensemble, color='b', alpha=0.2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: How does the DE approximation compare qualitatively to the MAP solution? In regions with training data? In regions without training data? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.2**: Compute the average log predictive density for the training and test set for the deep ensemble.\n",
    "\n",
    "*Hints*:\n",
    "- **Use equation (8) and (10)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: How does the three methods compare in terms of LPD for the toy data? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 5:  Modelling the Concrete dataset from the UCI repository\n",
    "\n",
    "In this part, we will test this model with three types of inference: MAP, LLLA and DE on a real dataset. We will work with the Concrete dataset from the UCI repository.\n",
    "\n",
    "The goal is to predict the strength of concrete based on a set of input features. See the link below for more information:\n",
    "\n",
    "[https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength](https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength)\n",
    "\n",
    "The dataset has $N = 1030$ observations in total and the input features are $D = 8$ dimensional.\n",
    "\n",
    "We will use a 90/10 split for training and testing.\n",
    "\n",
    "Let's load and prepare the data using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load to data using pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./concrete_data.csv')\n",
    "\n",
    "# prepare data\n",
    "X = df.to_numpy()\n",
    "y = X[:, -1]\n",
    "X = X[:, :-1]\n",
    "\n",
    "# sizes\n",
    "N, D = X.shape\n",
    "Ntest = int(0.1*N)\n",
    "\n",
    "# split into training and test\n",
    "np.random.seed(0)\n",
    "test_idx = np.random.choice(range(N), size=Ntest, replace=False)\n",
    "train_idx = np.setdiff1d(range(N), test_idx)\n",
    "\n",
    "X_train = X[train_idx, :]\n",
    "X_test = X[test_idx, :]\n",
    "y_train = y[train_idx][:, None]\n",
    "y_test = y[test_idx][:, None]\n",
    "\n",
    "# standardize input features\n",
    "Xm, Xs = np.mean(X_train, 0), np.std(X_train, 0)\n",
    "X_train = (X_train - Xm)/Xs\n",
    "X_test = (X_test - Xm)/Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is now to repeat the analysis from part 1-3 for this data set. \n",
    "\n",
    "Use $\\alpha = 1$, a step size of $10^{-2}$, a batch size of $50$, and $10000$ training iterations for these experiments and the same network architecture as above.\n",
    "\n",
    "**Task 5.1**: Fit a MAP-network to the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2**: Compute the training and test average LPD using the MAP solution.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.3**: Use the MAP solution to compute an LLLA approximation.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.4**: Compute the average training and test LPD for the LLLA solution.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.5**: Fit a deep ensemble with 10 models (it may take a little while to run, so start with a smaller number and increase the number of models when you are sure that everything is correct).\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.6**: Evaluate the training and test LPDs for the DE.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.7**: Which of the three methods (MAP, LLLA, DE) works best for this data set? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.8**: Plot the performance as a function of the number of models in the ensemble. Plot the performance of the MAP solution as a horizontal line for reference. How well does the DE compare in terms of training and test performance?\n",
    "\n",
    "*Hints*\n",
    "- *First compute predictions for an ensemble with $S = 1$ model (i.e. the MAP solution), then $S = 2$, $S = 3$ etc.*\n",
    "- *You only need to fit your ensemble once. After you've fitted the ensemble you can make predictions using subsets of the models using the `subset_models` argument to the `predict` function.*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02477",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
